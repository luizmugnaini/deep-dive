\chapter{Probability Theory}

\section{Probability}

\begin{definition}[Sample space \& events]
\label{def:sample-space}
We define the \emph{sample space} of an experiment to be the set \(\Omega\)
composed of all possible outcomes.

An \emph{event} is defined to be any subset \(A \subseteq \Omega\) of the sample
space. The event \(A\) is said to have occurred whenever the outcome inhabits
\(A\). Two events \(A, B \subseteq \Omega\) are said to be \emph{mutually
  exclusive} whenever \(A\) and \(B\) are disjoint.
\end{definition}

\begin{definition}[\(\sigma\)-algebra]
\label{def:sigma-algebra}
Let \(\Omega\) be a set. A family of subsets \(\Sigma \subseteq 2^{\Omega}\) is called
a \emph{\(\sigma\)-algebra} if it satisfies the following conditions:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item The empty set is an element of \(\Sigma\).
\item If \(A \in \Sigma\), then the complement \(\Compl{A}\) is an
  element of \(\Sigma\).
\item If \(\{A_j\}_{j \in J}\) is a collection of elements of \(\Sigma\)
  indexed by a \emph{countable set} \(J\), then the union
  \(\bigcup_{j \in J} A_j\) is also contained in \(\Sigma\).
\end{enumerate}
From condition (b) it follows immediately that \(\Omega \in \Sigma\).
\end{definition}

\begin{definition}[Probability function]
\label{def:probability-function}
Given a sample space \(\Omega\) and an associated \(\sigma\)-algebra \(\Sigma\),
we define a \emph{probability function} on \(\Sigma\) to be a map
\(\Prob: \Sigma \to \R\) such that
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item The map \(\Prob\) is non-negative.
\item The probability of the whole sample space is \(1\), that is,
  \(\Prob(\Sigma) = 1\).
\item Given a \emph{countable set} of pairwise \emph{disjoint} events
  \(\{A_j\}_{j} \subseteq \Sigma\), then
  \[
  \Prob\Big( \bigcup_{j \in J} A_j \Big) = \sum_{j \in J} \Prob(A_j).
  \]
\end{enumerate}
\end{definition}

\begin{lemma}
\label{lem:probability-function}
Let \(\Omega = \{s_j\}_{j \in J}\) be a countable sample space and \(\Sigma\) be
an associated \(\sigma\)-algebra. If \(\Prob: \Sigma \to \R\) is a mapping
associated with non-negative real numbers \(\{p_j\}_{j \in J}\) with
\(\sum_{j \in J} p_j = 1\), for which
\[
\Prob(A) \coloneq \sum_{j \colon s_j \in A} p_j
\]
for each \(A \in \Sigma\), then \(\Prob\) is a probability function on \(\Sigma\).
\end{lemma}

\begin{theorem}
\label{thm:probability-calculus-basics}
Let \(\Sigma\) be a \(\sigma\)-algebra associated to a sample space \(\Omega\),
and \(\Prob: \Sigma \to \R\) be a probability function. If \(A, B \in \Sigma\) are
any sets then the following is holds:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item \(\Prob(\Compl{A}) = 1 - \Prob(A)\).
\item \(\Prob(\emptyset) = 0\).
\item \(\Prob(A) \leq 1\), therefore \(\Prob(\Sigma) \subseteq [0, 1]\).
\item \(\Prob(B \cap \Compl{A}) = \Prob(B) - \Prob(A \cap B)\).
\item \(\Prob(A \cup B) = \Prob(A) + \Prob(B) - \Prob(A \cap B)\), hence
  \(\Prob(A \cup B) \geq \Prob(A) + \Prob(B) - 1\), which is known as the
  \emph{Bonferroni's inequality}.
\item If \(A \subseteq B\) then \(\Prob(A) \leq \Prob(B)\).
\end{enumerate}
Moreover, if \(\{C_{j}\}_{j \in J}\) is a countable partition of \(\Sigma\), and
\(\{A_i\}_{i \in \N}\) is any family of elements of \(\Sigma\), we also have
\begin{enumerate}[(a)]\setlength\itemsep{0em}\setcounter{enumi}{6}
\item \(\Prob(A) = \sum_{j \in J} \Prob(A \cap C_j)\).
\item \(\Prob(\bigcup_{i \in \N} A_i) \leq \sum_{i \in \N} \Prob(A_i)\).
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item Notice that \(A \cup \Compl{A} = \Sigma\), therefore \(\Prob(A \cup
  \Compl{A}) = \Prob(A) + \Prob(\Compl{A}) = 1\), which proves the statement.

\item Since \(\Compl{\Sigma} = \emptyset\), then
  \(\Prob(\emptyset) = 1 - \Prob(\Sigma) = 0\).

\item Since \(\Prob\) is a non-negative map, then \(\Prob(\Compl{A}) \geq 0\),
  hence \(\Prob(A) = 1 - \Prob(\Compl{A})\) implies in \(\Prob(A) \leq 1\).

\item Notice that in general \(B = (B \cap A) \cup (B \cap \Compl{A})\),
  therefore \(\Prob(B) = \Prob(B \cap A) + \Prob(B \cap \Compl{A})\), from which
  the formula follows.

\item Since \(A \cup B = A \cup (B \cap \Compl A)\), and from the fact that the
  sets \(A\) and \(B \cap \Compl A\) are disjoint, then
  \(\Prob(A \cup B) = \Prob(A) + \Prob(B \cap \Compl A)\). Using the result from
  the last item for \(\Prob(B \cap \Compl A)\) we obtain the required
  formula. For the inequality, it suffices to see that \(\Prob(A \cap B) \leq 1\).

\item Since \(A \cap B = A\) then by the result of item (d) we obtain
  \[
  \Prob(A) = \Prob(A \cap B) = \Prob(B) - \Prob(B \cap \Compl A)
  \]
  and since \(\Prob(B \cap \Compl A) \in [0, 1]\), then
  \(\Prob(A) \leq \Prob(B)\).

\item Since \(\bigcup_{j \in J} C_j = \Sigma\) we have
  \[
  A = A \cap \Sigma = A \cap \Big( \bigcup_{j \in J} C_j \Big)
  = \bigcup_{j \in J} A \cap C_j,
  \]
  therefore \(\Prob(A) = \Prob(\bigcup_{j \in J} A \cap C_j) = \sum_{j \in J}
  \Prob(A \cap C_j)\).

\item We shall construct a collection \(\{A'_{i}\}_{i \in \N}\) of disjoint sets
  partitioning \(\bigcup_{i \in \N} A_i\). To do so, define
  \(A'_{0} \coloneq A_{0}\) and for any other \(i \in \N_{> 0}\) we take
  \(A'_i \coloneq A_i \setminus \bigcup_{j = 0}^{i-1} A_j\). To see that such
  collection is indeed a partition, let \(i, j \in \N\) be any two distinct indices and
  notice that
  \begin{align*}
    A_i' \cap A_j'
    &= \Big( A_i \setminus \bigcup_{k=0}^{i-1} A_k \Big)
    \cap \Big( A_j \setminus \bigcup_{k=0}^{j-1} A_k \Big) \\
    &= \Big( A_i \cap \Big(\bigcup_{k=0}^{i-1} A_k\Big)^{\text{c}} \Big)
      \cap \Big( A_j \cap \Big(\bigcup_{k=0}^{j-1} A_k\Big)^{\text{c}} \Big) \\
    &= \Big( A_i \cap \Big(\bigcap_{k=0}^{i-1} \Compl A_k\Big) \Big)
      \cap \Big( A_j \cap \Big(\bigcap_{k=0}^{j-1} \Compl A_k\Big) \Big),
  \end{align*}
  from which, if \(i > j\) then
  \(\Compl A_j \subseteq A_i \cap \bigcap_{k=0}^{i-1} A_k\), thus the first term
  is disjoint from the second---the case for \(i < j\) is symmetric. Since our
  new collection satisfies the pairwise disjoint condition, we find
  \(\Prob(\bigcup_{i \in \N} A_i) = \sum_{i \in \N} \Prob(A'_i)\). From our
  construction we know that \(A'_i \subseteq A_i\) thus \(\Prob(A'_i) \leq
  \Prob(A_i)\) and hence \(\sum_{i \in \N} \Prob(A_i') \leq \sum_{i \in \N}
  \Prob(A_i)\), which proves the statement.
\end{enumerate}
\end{proof}

\begin{definition}[Conditional probability]
\label{def:conditional-probability}
Let \(A\) and \(B\) be events in a sample space \(\Omega\), with
\(\Prob(B) > 0\) ---where \(\Prob\) is a probability function. We define the
\emph{conditional probability of \(A\) given \(B\)} to be
\[
\Prob(A \mid B) = \frac{\Prob(A \cap B)}{\Prob(B)}.
\]
In this case, the map \(\Prob(- \mid B): \Sigma \to \R\) is a probability
function associated to \(\Prob\).
\end{definition}

Moreover, by symmetry we have \(\Prob(B \mid A) = \Prob(A \cap B)/\Prob(A)\),
therefore one obtains
\[
\Prob(A \mid B) = \Prob(B \mid A) \frac{\Prob(A)}{\Prob(B)}.
\]
When \(A\) and \(B\) are unrelated events---that is, \(A \cap B = \emptyset\)
---one has both \(\Prob(A \mid B) = \Prob(B \mid A) = 0\) since
\(\Prob(A \cap B) = 0\).

\begin{theorem}[Bayes' rule]
\label{thm:bayes-rule}
Let \(\{A_{j}\}_{j \in J}\) be a countable partition of the sample space, and
let \(B\) be any event. Then, for each \(j \in J\) on has
\[
\Prob(A_j \mid B) =
\frac{\Prob(B \mid A_j) \Prob(A_j)}{\sum_{j \in J} \Prob(B \mid A_j) \Prob(A_j)}.
\]
\end{theorem}

\begin{definition}[Statistically independent events]
\label{def:statistically-independent}
A pair of events \(A\) and \(B\) is said to be statistically independent from
each other if it is the case that \(\Prob(A \cap B) = \Prob(A) \Prob(B)\).
\end{definition}

\begin{theorem}
\label{thm:statistically-independent-pairs}
If \(A\) and \(B\) are statistically independent events, then the following
pairs of events are also independent:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item \(A\) and \(\Compl{B}\).
\item \(\Compl{A}\) and \(B\).
\item \(\Compl{A}\) and \(\Compl{B}\).
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item One has
  \begin{align*}
    \Prob(A \cap \Compl B)
    &= \Prob(A) - \Prob(A \cap B) \\
    &= \Prob(A) - \Prob(A) \Prob(B) \\
    &= \Prob(A) (1 - \Prob(B)) \\
    &= \Prob(A) \Prob(\Compl B).
  \end{align*}
\item The symmetric argument can be applied to the case of \(\Compl A\) and \(B\).

\item For both complements, we have
  \begin{align*}
    \Prob(\Compl A \cap \Compl B)
    &= \Prob(\Compl{(A \cup B)}) \\
    &= 1 - \Prob(A \cup B) \\
    &= 1 - (\Prob(A) + \Prob(B) - \Prob(A \cap B)) \\
    &= 1 - \Prob(A) - \Prob(B) + \Prob(A) \Prob(B) \\
    &= (1 - \Prob(A)) (1 - \Prob(B)) \\
    &= \Prob(\Compl A) \Prob(\Compl B).
  \end{align*}
\end{enumerate}
\end{proof}

\begin{remark}
\label{rem:event-independence}
Let \(A\), \(B\) and \(C\) be three events in our sample space such that
\[
\Prob(A \cap B \cap C) = \Prob(A) \Prob(B) \Prob(C).
\]
\emph{It is not necessarily true} that \(A\), \(B\) and \(C\) are pairwise
disjoint---that is, although the probability of them occurring simultaneously
splits, it does not mean that the events are pairwise independent! Moreover, one
\emph{cannot} define the independence of the events \(A\), \(B\) and \(C\) by
requiring pairwise independence without running into problems---unfortunately
the generalisation of \cref{def:statistically-independent} is not that simple,
but here we come to the rescue.
\end{remark}

\begin{definition}[Mutually independent events]
\label{def:mutually-independent-events}
Let \((A_1, \dots, A_n)\) be a collection of events. We say that they are
\emph{mutually independent} if for any subcollection \((A_{j_1}, \dots,
A_{j_k})\) one has
\[
\Prob \Big(\bigcap_{i=1}^k A_{j_i}\Big) = \prod_{i=1}^k \Prob(A_{j_i}).
\]
\end{definition}

\section{Random Variables}

\begin{definition}[Random variable]
\label{def:random-variable}
A \emph{random variable} is a map \(X: \Omega \to \mathcal{T}\), where
\(\Omega\) is our ambient sample space and \(\mathcal{T}\) is the \emph{target
  space}---for instance, \(\R^n\). This random variable induces a new sample space
\(\mathcal{X} = X(\Omega)\), on which we can define a new probability function.

Suppose that \(\Omega\) is countable. If \(\Sigma\) and \(\Sigma_X\) are
\(\sigma\)-algebras associated to \(\Omega\) and \(\mathcal{X}\) respectively,
then we define \(\Prob_X: \Sigma_X \to \R\) given by
\[
\Prob_X(X = x) \coloneq \Prob(\{y \in \Omega \colon X(y) = x\}),
\]
where \(\Prob: \Sigma \to \R\) is a probability function. To ease the notation,
we shall merely write \(\Prob(X = x)\) rather than \(\Prob_X(X = x)\).

Now, if \(\Omega\) is an uncountable sample space, we define \(\Prob_X\) as
follows: for any \(A \subseteq \mathcal{X}\) let
\[
\Prob_X(X \in A) \coloneq \Prob(\{s \in \Omega \colon X(s) \in A\}).
\]
\end{definition}

\section{Distribution Functions}

\begin{definition}[Cumulative distribution function]
\label{def:cumulative-distribution-function}
Let \(X: \Omega \to \R\) be a random variable. We define a \emph{cumulative distribution
  function} (cdf) associated to \(X\) to be map \(F_X: \Omega \to \R\) given by
\[
F_X(x) \coloneq \Prob_X(X \leq x)
\]
for each \(x \in \Omega\). From its construction, \(F_X\) is a
\emph{right-continuous} mapping. We shall use the expression \(X \sim F_X\) to
encode ``\(X\) has a distribution given by \(F_X\)''.
\end{definition}

\begin{theorem}[Cdf necessary and sufficient properties]
\label{thm:cdf-iff-properties}
A map \(F: \Omega \to \R\) is a cumulative distribution if and only if all of
the following three conditions hold:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item We have limits \(\lim_{x \to -\infty} F(x) = 0\) and \(\lim_{x \to \infty} F(x) = 1\).
\item The map \(F\) is non-decreasing.
\item The map \(F\) is right-continuous, that is, for any \(x_0 \in \Omega\) we
  have \(\lim_{x \to +x_0} F(x) = F(x_0)\).
\end{enumerate}
\end{theorem}

\begin{example}
\label{exp:cdfs}
Here we list some of the most important cumulative distribution functions:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item Let \(p\) be the probability of success and \(X\) be the number of trials
  required to obtain a success. The probability functions associated with \(X\)
  is \(\Prob(X = x) = (1 - p)^{x-1} p\) and its corresponding cdf is called a
  \emph{geometric distribution}:
  \[
  F_X(x) = 1 - (1 - p)^x.
  \]

% \item The \emph{logistic distribution} is defined to be the cdf given by
%   \[
%   F_X(x) = \frac{1}{1 + e^{-x}}.
%   \]
\end{enumerate}
\end{example}

\begin{definition}
\label{def:continuity-discreteness-random-variable}
A random variable \(X\) is said to be \emph{continuous} if its associated
cumulative distribution function \(F_X\) is continuous. On the other hand, we
say that \(X\) is \emph{discrete} if \(F_X\) is a step-function.
\end{definition}

\begin{definition}
\label{def:identically-distributed-random-variables}
Let \(\Sigma^1\) be the smallest \(\sigma\)-algebra containing all intervals of
real numbers. Two random variables \(X\) and \(Y\) are said to be
\emph{identically distributed} if for some \(A \in \Sigma^1\), one has
\(\Prob(X \in A) = \Prob(Y \in A)\). We denote that \(X\) and \(Y\) have
identical distributions by \(X \sim Y\).
\end{definition}

\begin{theorem}
\label{thm:random-variables-identical-iff-equal-cdfs}
Two random variables \(X\) and \(Y\) are identically distributed if and only if
\(F_X = F_Y\).
\end{theorem}

\section{Density \& Mass Functions}

\begin{definition}[Probability mass function]
\label{def:probability-mass-function}
The \emph{probability mass function} (pmf) of a \emph{discrete} random variable
\(X\) is a map \(f_X: \Omega \to \R\) given by
\[
f_X(x) = \Prob(X = x).
\]
\end{definition}

\begin{definition}[Probability density function]
\label{def:probability-mass-function}
The \emph{probability density function} (pdf) of a \emph{continuous} random variable
\(X\) is a map \(f_X: \Omega \to \R\) for which
\[
F_X(x) = \int_{-\infty}^x f_X(t)\, \diff t.
\]
Consequently, for any \(a < b\) we have
\[
\Prob(a \leq X \leq b) = \int_a^b f_X(t)\, \diff t.
\]
\end{definition}

\begin{remark}
\label{rem:interval-probability-continuous-random-variable}
In the case of a continuous random variable \(X\) we have \(\Prob(X = x) = 0\)
for any \(x\) of the sample space, therefore one has for any interval:
\[
\Prob(a < X < b)
= \Prob(a < X \leq b)
= \Prob(a \leq X < b)
= \Prob(a \leq X \leq b).
\]
\end{remark}

\begin{theorem}
\label{thm:}
A map \(f_X\) is a pdf (or pmf) of a random variable \(X\) if and only if the
following requirements are met:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item The map \(f_X\) is non-negative.
\item In the case of a pdf, \(\int_{-\infty}^{\infty} f_X(x)\, \diff x = 1\). On
  the other hand, for a pmf, \(\sum_x f_X(x) = 1\).
\end{enumerate}
\end{theorem}

\begin{notation}
\label{not:univariate-multivariate-distributions}
Distributions do not need to depend on a single random variable, when they do we
call them \emph{univariate}, otherwise \emph{multivariate} distributions. For
instance, a multivariate real-valued random variable \(X\) is a map
\(X: \Omega \to \R^n\) having an associated cumulative distribution function
and---if existent---probability density functions:
\[
F_X(x) = \Prob(X_1 \leq x_1, \dots, X_n \leq x_n)
= \int_{-\infty}^{x_1} \cdots \int_{-\infty}^{x_n} f_X(x_1, \dots, x_n)\,
\diff x_1 \dots \diff x_n
\]
\end{notation}

\begin{definition}[Joint probability]
\label{def:joint-probability}
Let \((X_j: \Omega_j \to \R)_{j=1}^m\) be a collection of discrete random variables. We
define the \emph{joint probability} of this collection by the probability mass
function \(f: \prod_{j=1}^m \Omega_j \to \R\) given by
\[
f(x_1, \dots, x_m)
= \Prob(X_1 = x_1, \dots, X_m = x_m)
= \frac{n(x_1, \dots, x_m)}{N}
\]
where \(n(x_1, \dots, x_m)\) is the number of events with state \(x_1, \dots,
x_m\), and \(N\) is the total number of events. The probability of \(X_j = x_j\)
irrespective of the other random variables as the \emph{marginal probability} of
\(X_j\) and sometimes denote it by \(p(x_j)\). Fixing that \(X_i = x_i\) for
each \(i \neq j\) we can calculate the \emph{conditional probability} of \(X_j
= x_j\), which we write as \(f(x_j \mid x_1, \dots, x_m)\).
\end{definition}

\begin{definition}[Categorical variables]
\label{def:categorical-variables}
Variables that take a finite set of \emph{unordered} values are called
\emph{categorical variables}, and are used widely in machine learning contexts.
\end{definition}

\begin{lemma}[Sum rule]
\label{lem:joint-probability-sum-rule}
Let \(X: \Omega_X \to \mathcal{X}\) and \(Y: \Omega_Y \to \mathcal{Y}\) be
multivariate random variables and consider the joint probability
\(\Prob: \Omega_X \times \Omega_Y \to \R\). The \emph{sum rule}
states that the marginal probability of \(X = x\) is given by
\[
\Prob(x) =
\begin{cases}
  \sum_{y \in \mathcal{Y}} \Prob(x, y), &\text{if } y \text{ is discrete} \\
  \int_{\mathcal{Y}} \Prob(x, y)\, \diff y, &\text{if } y \text{ is continuous}
\end{cases}
\]
\end{lemma}

\begin{lemma}[Product rule]
\label{lem:joint-probability-product-rule}
Given a joint probability \(\Prob: \Omega_X \times \Omega_Y \to \R\), the product
rule states that
\[
\Prob(x, y) = \Prob(y \mid x) \Prob(x).
\]
\end{lemma}

From the product rule we can obtain Bayes' theorem by noting the symmetry
\(\Prob(x, y) = \Prob(x \mid y) \Prob(y)\), resulting in
\[
\Prob(x \mid y) = \frac{\Prob(y \mid x) \Prob(x)}{\Prob(y)}.
\]
In the eyes of machine learning and Bayesian statistics, this formula gives a
way to make inferences about the unobserved random variable \(X\) by having a
\emph{prior} knowledge \(\Prob(x)\) and a second random variable \(Y\) of which we
can observe and have a \emph{marginal evidence} \(\Prob(y)\) and a relational
\emph{likelihood} \(\Prob(y \mid x)\). This data gives a \emph{posterior} knowledge
\(\Prob(x \mid y)\).

\begin{remark}
\label{rem:likelihood}
The value \(\Prob(y \mid x)\) can be called either the ``likelihood of \(x\) given
\(y\)'' or the ``probability of \(y\) given \(x\)''.
\end{remark}

\section{Means \& Covariances}

\begin{definition}[Expected value]
\label{def:expected-value}
The \emph{expected value} of a function \(g: \R \to \R\) of a \emph{univariate
continuous} random variable \(X: \Omega_X \to \mathcal{X}\) with distribution \(f_X\)
is defined as
\[
\Expect_X g \coloneq \int_{\mathcal{X}} g(x) f_X(x)\, \diff x.
\]
Analogously, if \(Y: \Omega_Y \to \mathcal{Y}\) is a \emph{univariate discrete}
random variable with \(Y \sim f_Y\), the expected value of \(g\) is given by
\[
\Expect_Y g = \sum_{y \in \mathcal{Y}} g(y) f_Y(y).
\]
As one might ``expect'', in the case of an \(n\)-multivariate random variable
\(Z: \Omega_Z \to \mathcal{Z}\) we have
\[
\Expect_Z g =
\begin{bmatrix}
  \Expect_{Z_1} g \\
  \vdots \\
  \Expect_{Z_n} g
\end{bmatrix}
\]
\end{definition}

\begin{definition}[Mean, median \& mode]
\label{def:mean-of-random-variable}
We define the \emph{mean} (or \emph{population} mean) of an \(n\)-multivariate
random variable \(X: \Omega \to \mathcal{X}\) to be
\[
\Mean_X = \Expect_X \Id =
\begin{bmatrix}
  \Expect_{X_1} \Id \\
  \vdots \\
  \Expect_{X_n} \Id
\end{bmatrix}
\]
where, as in \cref{def:expected-value} we have for each \(1 \leq j \leq n\):
\[
\Expect_{X_j} \Id =
\begin{cases}
  \int_{\mathcal{X}_j} x_j f_X(x_j)\, \diff x_j, &\text{if } X \text{ is
                                                   continuous} \\
  \sum_{x_j \in \mathcal{X}_j} x_j f_X(x_j), &\text{if } X \text{ is discrete}
\end{cases}
\]

The \emph{median} of a \emph{univariate} discrete random variable is the middle-most
value of the image of the variable. In the case of \emph{univariate} continuous random
variables, we define the median is defined to be the value where the cumulative
density function is \(1/2\).

The \emph{mode} of a discrete random variable is the value having the highest
frequency of occurrence. For continuous random variables, we define the mode as
the values corresponding to maximal critical points of the probability density
function---which may admit more than a single mode.
\end{definition}

\begin{definition}[Covariance, variance \& standard deviation: univariate case]
\label{def:covariance-univariate}
The \emph{covariance} between two univariate real-valued random variables \(X\)
and \(Y\) is given by
\begin{align*}
  \Cov(X, Y)
  &\coloneq \Expect_{X, Y}[(x - \Mean_X)(y - \Mean_Y)] \\
  &= \Expect_{X, Y}(x y) - \Mean_X \Mean_Y.
\end{align*}
The \emph{variance} of the random variable \(X\) is defined to be
\begin{align*}
\Var X
&\coloneq \Cov(X, X) \\
&= \Expect_X[(x - \Mean_X)^2] \\
&= \Expect_X(x^2) - \Mean_X^2.
\end{align*}
The \emph{standard deviation} of \(X\) is defined as
\[
\stdev(X) \coloneq \sqrt{\Var(X)}.
\]
\end{definition}

\begin{definition}[Covariance \& variance: multivariate case]
\label{def:covariance-multivariate}
Let \(X\) and \(Y\) be multivariate random variables with \(\im X \subseteq
\R^n\) and \(\im Y \subseteq \R^m\). We define the \emph{covariance} (or \emph{cross-variance}) between
\(X\) and \(Y\) as
\[
\Cov(X, Y) \coloneq \Expect_{X, Y}(\langle x, y \rangle) -
\langle \Mean_X , \Mean_Y  \rangle
= \Cov(Y, X)^{\Transp} \in \R^n \times \R^m,
\]
where \(\langle -, - \rangle\) is the standard euclidean inner product.

As before, the variance of the multivariate random variable \(X\) is given by
\begin{align*}
  \Var X
  &= \Cov(X, X) \\
  &= \Expect(\langle x - \Mean_X, x - \Mean_X \rangle) \\
  &= \Expect(\langle x, x \rangle) - \langle \Mean_X, \Mean_X \rangle \\
  &=
    \begin{bmatrix}
      \Cov(X_1, X_1) &\Cov(X_1, X_2) &\dots &\Cov(X_1, X_n) \\
      \vdots &\vdots &\dots &\vdots \\
      \Cov(X_n, X_1) &\Cov(X_n, X_2) &\dots &\Cov(X_n, X_n) \\
    \end{bmatrix}
\end{align*}
This matrix is symmetric and positive semi-definite.
\end{definition}

\begin{definition}[Correlation]
\label{def:correlation}
The \emph{correlation} between two random variables \(X\) and \(Y\) is given by
\[
\Correlation(X, Y) \coloneq \frac{\Cov(X, Y)}{\stdev(X) \stdev(Y)} \in [-1, 1].
\]
A positive correlation means that if \(x\) increases, then \(y\) also
increases. A negative correlation means that if \(x\) increases, then \(y\)
decreases.
\end{definition}

\begin{definition}[Empirical mean \& covariance]
\label{def:empirical-mean-and-covariance}
Let \((x_j)_{j=1}^{n}\) be a collection of observations (empirical data). We
define the \emph{empirical mean} as
\[
\overline x \coloneq \frac{1}{n} \sum_{j=1}^n x_j.
\]
Analogously, the \emph{empirical covariance} is defined as
\[
\Cov(x_j)_{j=1}^n
= \frac{1}{n} \sum_{j=1}^n \langle x_j - \overline x, x_j - \overline x \rangle.
\]
\end{definition}

\subsection{Random Variables \& Linear Maps}

\begin{lemma}
\label{lem:sums-differences-random-variables}
Given a pair of random variables \(X\) and \(Y\) with states in \(\R^n\), we can
compute the following means:
\begin{align*}
  \Mean_{X + Y} &= \Mean_X + \Mean_Y, \\
  \Mean_{X - Y} &= \Mean_X - \Mean_Y,
\end{align*}
and variances:
\begin{align*}
  \Var(X + Y) &= \Var X + \Var Y + \Cov(X, Y) + \Cov(Y, X), \\
  \Var(X - Y) &= \Var X + \Var Y - (\Cov(X, Y) + \Cov(Y, X)). \\
\end{align*}
\end{lemma}

Let \(Y = A X + b\) be a linear transformation and \(X\) a random variable, then
\(Y\) itself is a random variable. The following relations hold true for the
mean of \(Y\):
\[
\Mean_Y = \Expect_Y Y = \Expect_X[A X + b] = A \Expect_X X + b = A \Mean_X + b
\]
Moreover, we also have the following relation with the variance of \(Y\):
\begin{align*}
  \Var(Y)
  &= \Var(A X + b) \\
  &= \Cov(A X + b, A X + b) \\
  &= \Expect[(A X + b)(A X + b)^{\Transp}] - \Mean_{A X + b} \Mean_{A X +
    b}^{\Transp} \\
  &= \Expect[
    A X X^{\Transp} A^{\Transp} + A X b^{\Transp} + b X^{\Transp} A^{\Transp}
    + b b^{\Transp}
    ]
    - (A \Mean_X \Mean_X^{\Transp} A^{\Transp} + A \Mean_X b^{\Transp}
    + b \Mean_X^{\Transp} A^{\Transp} - b b^{\Transp}) \\
  &= A \Expect[X X^{\Transp}] A^{\Transp} - A \Mean_X \Mean_X^{\Transp}
    A^{\Transp} \\
  &= \Var(A X) \\
  &= A \Var(X) A^{\Transp}
\end{align*}
Finally, we can compute the covariance of \(X\) and \(Y\) as
\begin{align*}
  \Cov(X, Y)
  &= \Expect[X (A x + b)^{\Transp}] - \Mean_X \Mean_{A X + b}^{\Transp} \\
  &= \Mean_X b^{\Transp} + \Expect[X X^{\Transp}] A^{\Transp}
    - \Mean_X b^{\Transp} - \Mean_X \Mean_X^{\Transp} A^{\Transp} \\
  &= \Expect[X X^{\Transp}] A^{\Transp} - \Mean_X \Mean_X^{\Transp} A^{\Transp} \\
  &= \Var(X) A^{\Transp}
\end{align*}

\begin{lemma}
\label{lem:statistical-independence-and-cov-var}
If \(X\) and \(Y\) are statistically independent random variables, then
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item \(\Var(X + Y) = \Var X + \Var Y\).
\item \(\Cov(X, Y) = 0\).
\end{enumerate}
It is to be noted that two random variables may have a zero covariance but
still be statistically dependent---this is because covariance merely measures
linear dependence.
\end{lemma}

\begin{definition}[Conditional independence]
\label{def:conditional-independence}
Two random variables \(X\) and \(Y\) are said to be conditionally independent of
a given random variable \(Z\) when
\[
\Prob(x, y \mid z) = \Prob(x \mid z) \Prob(y \mid z)
\]
for all states \(z\) of \(Z\). If we use
\cref{lem:joint-probability-product-rule} we obtain \(\Prob(x, y \mid z) =
\Prob(x \mid y, z) \Prob(y \mid z)\) and hence the conditional independence can
be reformulated as
\[
\Prob(x \mid y, z) = \Prob(x \mid z).
\]
\end{definition}

\begin{definition}[Inner product of random variables]
\label{def:inner-product-random-variables}
Given two random variables \(X\) and \(Y\), we can define their inner product to
be
\[
\langle X, Y \rangle \coloneq \Expect(X Y).
\]
Also if it is the case that \(\Mean_X = 0 = \Mean_Y\), then in particular
\(\langle X, Y \rangle = \Cov(X, Y)\) and we would have
\[
\Correlation(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var(X) \Var(Y)}}
= \frac{\langle X, Y \rangle}{\| X \|\, \| Y \|} = \cos \theta,
\]
where \(\theta\) is the angle between the vectors \(X\) and \(Y\).
\end{definition}

\section{Distributions}

\begin{definition}[Gaussian distribution]
\label{def:gaussian-distribution}
Let \(X\) be a \emph{univariate} random variable with mean \(\Mean\) and
standard deviation \(\stdev\). The \emph{Gaussian distribution} of \(X\) has a
density function given by
\[
N(x \mid \Mean, \stdev^2) \coloneq \frac{1}{\sqrt{2 \pi \sigma^{2}}}
\exp\Big( -\frac{(x - \Mean)^2}{2 \stdev^2} \Big).
\]
Now, if \(X\) where an \emph{\(n\)-multivariate} random variable, then the Gaussian
distribution of \(X\) would have a density function defined as
\[
N(x \mid \Mean, \Var X) \coloneq \frac{1}{\sqrt{(2 \pi)^n \det(\Var X)}}
\exp\Big(-\frac{1}{2} (x - \Mean)^{\Transp} \Var(X)^{-1} (x - \Mean)\Big).
\]

The special case where \(\Mean = 0\) and \(\Var(X) = \Id\) is referred to as the
\emph{standard normal distribution}.
\end{definition}

Let \(X\) and \(Y\) be multivariate random variables and consider the
Gaussian distribution
\[
f(x, y) = N\bigg(
\begin{bmatrix}
  \Mean_X \\
  \Mean_Y
\end{bmatrix},
\begin{bmatrix}
  \Var X     & \Cov(X, Y) \\
  \Cov(Y, X) & \Var Y
\end{bmatrix}
\bigg)
\]


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../deep-dive"
%%% End:
