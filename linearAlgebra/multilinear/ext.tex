\section{Alternating Tensors and Exterior Powers}

\subsection{Permutations, Transpositions and Sign}

\begin{definition}[Transposition]
  \label{def: transposition}
  We define a transposition on a collection \(\{1, \dots, n\}\) to be a map
  \(\tau \in \mathcal S_n\) such that exists indices \(1 \leq i < j \leq n\)
  for which \(\tau(i) = j\) and \(\tau(j) = i\), and \(\tau(k) = k\) for all
  \(k \neq i, j\).
\end{definition}

\begin{definition}[Elementary transpositions]
  Let \(\tau \in \mathcal S_n\) be a transposition. We say that \(\tau\) is an
  elementary transposition if exists \(i \in \{1, \dots, n\}\) for which
  \(\tau(i) = i + 1\) and \(\tau(i + 1) = 1\), and for all \(j \neq i\) we have
  \(\tau(j) = j\).
\end{definition}

\begin{proposition}\label{prop: permutations to transpositions}
  Every permutation can be written as a composition of finitely many
  transpositions.
\end{proposition}

\begin{proof}
  We proceed via induction on the number of points of \(\{1, \dots, n\}\). For
  the base case \(n = 2\) the composition is trivial. Assume as the induction
  hypothesis that for \(n - 1 > 2\) the statement is true. Now, consider
  \(\sigma \in \mathcal S_n\) and \(i \in \{1, \dots, n\}\) be any element in
  the ordered collection \(I_n := \{1, \dots, n\}\). Denote \(\tau_{i, j}\) the
  transposition that changes \(i\) with \(j\) and maintains unchanged the
  remainder of the points. Assume that \(\sigma(i) = j\), then clearly
  \(\tau_{i, j} \sigma(i) = \tau_{i, j}(j) = i\). Notice that since \(\tau_{i,
  j} \sigma\) maintains \(i\) unchanged, we can see it as a permutation of \(n
  - 1\) points (by simply ignoring the point \(i\)), hence it can be written as
  a composition of finitely many transpositions by the induction hypothesis.
  Notice that since \(\sigma = \tau_{i, j} (\tau_{i, j} \sigma)\) we find that
  \(\sigma\) can also be written as a composition of finitely many
  transpositions.
\end{proof}

\begin{proposition}
  Every transposition can be written as a composition of finitely many
  elementary transpositions.
\end{proposition}

\begin{corollary}
  Every permutation can be written as a composition of finitely many elementary
  transpositions.
\end{corollary}

\begin{definition}[Sign]
  \label{def: sign}
  Let \(\sigma \in \mathcal S_n\) be a permutation on \(n\) objects. We define
  the sign of \(\sigma\) as a map \(\sign: \mathcal S_n \to
  \{1, -1\}\) such that
  \[
    \sign(\sigma) = (-1)^m, \text{ where }
    m := |\{(i, j) : 1 \leq i < j \leq n,\ \sigma(i) > \sigma(j)\}|.
  \] 
  Equivalently, if \(x_1, \dots, x_n \in k\) then we can define the
  sign of \(\sigma\) as
  \[
    \sign(\sigma) = \prod_{i < j} \frac{x_{\sigma(i)} -
    x_{\sigma(j)}}{x_i - x_j}.
  \] 
\end{definition}

\begin{corollary}
  If \(\sigma\) can be written as an odd number of transpositions, then
  \(\sign(\sigma) = -1\). Otherwise, if it is an even number of
  transpositions, we have \(\sign(\sigma) = 1\).
\end{corollary}

\begin{proposition}\label{prop: sign is a group homomorphism}
  Let \(\sigma, \tau \in \mathcal S_n\), then 
  \[
    \sign(\sigma \tau) = \sign(\sigma)
    \sign(\tau).
  \] 
  This implies that \(\sign: \mathcal S_n \to \{1, -1\}\) is a
  group homomorphism.
\end{proposition}

\begin{proof}
  Consider the compostion \(\sigma \tau\), then we have
  \begin{equation}\label{eq: sign sigma tau}
     \sign(\sigma \tau) = \prod_{i < j} \frac{x_{\sigma\tau(i)} -
     x_{\sigma\tau(j)}}{x_i - x_j}.
  \end{equation}
  Define the following: when \(\tau(i) < \tau(j)\) then \(\tau(i) := p\) and
  \(\tau(j) := q\); on the other hand, when \(\tau(j) < \tau(i)\) let \(\tau(i)
  := q\) and \(\tau(j) := p\). This way we find that
  \begin{equation}\label{eq: sigma term}
    \frac{x_{\sigma\tau(i)} - x_{\sigma\tau(j)}}{x_{\tau(i)} - x_{\tau(j)}}
    = \frac{x_{\sigma(p)} - x_{\sigma(q)}}{x_p - x_q}.
  \end{equation}
  From construction of the above, \cref{eq: sigma term} yields
  \begin{equation}\label{eq: sign sigma}
    \sign(\sigma) = \prod_{p < q} \frac{x_{\sigma(p)} -
    x_{\sigma(q)}}{x_p - x_q}.
  \end{equation}
  Notice that we can write \cref{eq: sign sigma tau} (using \cref{eq: sign
  sigma}) as the product
  \[
    \sign(\sigma \tau) = \prod_{i < j}
    \frac{x_{\sigma \tau(i)} - x_{\sigma \tau(j)}}{x_{\tau(i)} - x_{\tau(j)}}
    \frac{x_{\tau(i)} - x_{\tau(j)}}{x_i - x_j}
    = \sign(\sigma) \sign(\tau)
  \]
  as wanted.
\end{proof}

\begin{definition}[Alternating map]
  \label{def: alternating map}
  Let vector spaces \(V, L\). A multilinear map \(f: V^n \to W\) is called
  alternating if for any \((v_1, \dots, v_n) \in V^n\) such
  that exists \(i < j\) for which \(v_i = v_j\) then
  \[
    f(v_1, \dots, v_n) = 0.
  \] 
\end{definition}

\begin{proposition}\label{prop: alternating map property}
  Let \(f: V^n \to W\) be an alternating multilinear map. Then if \(\tau \in
  \mathcal S_n\) is a transposition, we have
  \[
    f(v_{\tau(1)}, \dots, v_{\tau(n)}) = - f(v_1, \dots, v_n).
  \] 
  In general, if \(\sigma \in \mathcal S_n\) is any permutation, then
  \[
    f(v_{\sigma(1)}, \dots, v_{\sigma(n)}) = \sign(\sigma) f(v_1,
    \dots, v_n).
  \] 
\end{proposition}

\begin{proof}
  Let \(\tau\) be a transposition between indices \(i < j\), then consider
  \begin{align*}
    0 = f(v_1, \dots, v_i + v_j, \dots, v_i + v_j, \dots, v_n)
    =\, & f(v_1, \dots, v_i, \dots, v_i + v_j, \dots, v_n) \\
      & + f(v_1, \dots, v_j, \dots, v_i + v_j, \dots, v_n)
  \end{align*}
  therefore we find
  \begin{align*}
    f(v_1, \dots, v_i, \dots, v_i + v_j, \dots, v_n) 
    &= - f(v_1, \dots, v_j, \dots, v_i + v_j, \dots, v_n) \\
    f(v_1, \dots, v_i, \dots, v_j, \dots, v_n)
    &= - f(v_1, \dots, v_j, \dots, v_i, \dots, v_n) \\
    f(v_1, \dots, v_n)
    &= f(v_{\tau(1)}, \dots, v_{\tau(n)})
  \end{align*}
  where we used that \(f(v_1, \dots, v_i, \dots, v_i, \dots, v_n) = f(v_1,
  \dots, v_j, \dots, v_j, \dots, v_n) = 0\). This shows the first proposition.
  For the second proposition, by means of \cref{prop: sign is a group
  homomorphism} and \cref{prop: permutations to transpositions} and the above
  proposition for transpositions, we conclude the proof.
\end{proof}

\begin{remark}
  Notice that \cref{prop: alternating map property} is not sufficient to
  characterize alternating maps, it is but a necessary property.
\end{remark}

\subsection{Alternating Tensor and Exterior Powers}

\begin{definition}[Alternating tensor]
  \label{def: alternating tensor}
  Let \(V\) be a \(k\)-vector space we define a tensor \(T \in T_0^q(V)\) to be
  alternating if for all \(\sigma \in \mathcal S_q\) permutation we have
  \[
    f_\sigma(T) = \sign(\sigma) T.
  \] 
  We denote by \(\Lambda^q(V)\) the subspace of \(T_0^q(V)\) of alternating
  tensors and call it the \(q\)-exterior power of \(V\).
\end{definition}

\begin{proposition}[Alternating tensor projection]
  \label{prop: alternating projection}
  Let \(V\) be a \(k\)-vector space and \(\operatorname{char} k \nmid q!\). We
  define the linear projection operator \(A: T_0^q(V) \to T_0^q(V)\) where
  \[
    A(T) = \frac{1}{q!} \sum_{\sigma \in \mathcal S_q} \sign(\sigma)
    f_\sigma(T)
  \] 
  Then \(A^2 = A\) and \(\im A = \Lambda^q(V)\).
\end{proposition}

\begin{proof}
  First we show that \(\im A \subseteq \Lambda^q(V)\) (we already know that
  clearly \(\Lambda^q(V) \subseteq \im A\)). Notice that
  \begin{align*}
    f_\sigma(A(T)) 
    &= f_\sigma \left( \frac 1 {q!} \sum_{\tau \in \mathcal S_q}
    \sign(\tau) f_\tau(T) \right) \\
    &= \frac 1 {q!} \sum_{\tau \in \mathcal S_q} \sign(\tau)
    f_{\sigma \tau}(T) \\
    &= \sign(\sigma) \left( \frac 1 {q!} \sum_{\tau \in \mathcal
    S_q} \sign(\sigma \tau) f_{\sigma \tau}(T) \right) \\
    &= \sign(\sigma) A(T)
  \end{align*}
  where we've used \cref{prop: sign is a group homomorphism}. Hence we conclude
  that \(\im A = \Lambda^q(V)\). For the second proposition, notice that
  \[
    A^2 = \frac 1 {q!^2} \sum_{\sigma, \tau \in \mathcal S_q}
    \sign(\sigma \tau) f_{\sigma \tau}
    = \frac 1 {q!} \sum_{\rho \in \mathcal S_q} \sign(\rho)
    f_\rho = A
  \]
  since any permutation \(\rho \in \mathcal S_q\) can be represented in \(q!\)
  different ways as a form of a product \(\sigma \tau\). This concludes the
  proof.
\end{proof}

\begin{definition}[Exterior multiplication]
  \label{def: exterior multiplication}
  Let \(V\) be a \(k\)-vector space and \(v_1 \otimes \dots \otimes v_q \in
  T_0^q(V)\). We define the exterior multiplication to be the map
  \[
    A(v_1 \otimes \dots \otimes v_q) := v_1 \wedge \dots \wedge v_q
  \] 
\end{definition}

\begin{proposition}[Universal property for exterior power]
  \label{prop: exterior power universal property}
  Let \(V\) be a \(k\)-vector space. For all \(k\)-vector spaces \(L\) together
  with an alternating multilinear map \(\mu: V^q \to L\), there exists a unique
  \(k\)-linear map \(\ell: \Lambda^q(V) \to L\) for which the diagram commutes
  \[
    \begin{tikzcd}
      V^q \ar[d, swap, "\otimes"] \ar[dr, "\mu"] \\
      T_0^q(V) \ar[d, swap, two heads, "A"] &L \\
      \Lambda^q(V) \ar[ur, dashed, swap, "\ell"]
    \end{tikzcd}
  \] 
\end{proposition}

\begin{proof}
  Since \(\Lambda^q(V)\) is a subspace of \(T_0^q(V)\), then we can use the
  universal property \cref{thm: universal property of tensor products}.
  %(Uniqueness) Let \(\ell, f: \Lambda^q(V) \to L\) be \(k\)-linear maps such
  %that \(\ell \circ A \circ \otimes = f \circ A \circ \otimes = \mu\). Let
  %\((v_1, \dots, v_q) \in V^q\) be any element, then since \(A \circ \otimes
  %(v_1, \dots, v_q) = v_1 \wedge \dots \wedge v_q\) we conclude that \(f(v_1
  %\wedge \dots \wedge v_q) = \ell(v_1 \wedge \dots \wedge v_q)\) and hence \(f =
  %\ell\).
  %(Existence) We must show that \(\ell\) is indeed a \(k\)-linear map. Notice
  %that making \(\ell \circ A \circ \otimes = \mu\) 
\end{proof}

Now we get some geometric motivation behind the algebraic structure of the
exterior power \(\Lambda^q(V)\). Alternating tensors \(v_1 \wedge \dots \wedge
v_q \in \Lambda^q(V)\) can be seen as \(q\)-dimensional oriented volume elements,
where by oriented we mean that the transposition of two edges, say \(v_j\) and
\(v_i\), implies in a change up to a minus sign of the value (the sign is what
creates the bridge between orientation and wedge product). Notice that when
there are equal edges, the volume becomes malformed and \(v_1 \wedge \dots
\wedge v_q = 0\).

\begin{proposition}
  Let \(V\) be a \(n\)-dimensional \(k\)-vector space, where
  \(\operatorname{char} k \neq 2\). Define \(\{e_i\}_{i=1}^n\) to be a base for
  \(V\). Then the exterior product \(e_{i_1} \wedge \dots \wedge e_{i_q} = 0\)
  if there exists \(i_a = i_b\) for some \(1 \leq a, b \leq q\).
\end{proposition}

\begin{proof}
  Since \(A\) is an alternating map, we can use \cref{def: alternating map} and
  conclude the proof.
\end{proof}

For the next proposition, we proceed in a similar fashion as in \cref{prop:
basis for symmetric power}.

\begin{proposition}[Exterior power \(\Lambda^q(V)\) basis]
  \label{prop: exterior power basis}
  Let \(\{e_i\}_{i=1}^n\) be a basis for the \(k\)-vector space \(V\). The
  factorizable tensors
  \[
    A(e_{i_1} \otimes \dots \otimes e_{i_q}) = e_{i_1} \wedge \dots \wedge
    e_{i_q}
  \] 
  with \(1 \leq i_1 < \dots < i_n \leq n\) form a basis for the subspace
  \(\Lambda^q(V)\).
\end{proposition}

\begin{proof}
  Let \(B := \{e_{i_1} \wedge \dots \wedge e_{i_q} : 1 \leq i_1 < \dots < i_q
  \leq n\}\). Since \(\{e_{i_1} \otimes \dots \otimes e_{i_q} : 1 \leq i_1 <
  \dots i_q \leq n\}\) generates \(T_0^q(V)\) (see \cref{lem: tensor basis}),
  then clearly \(B\) does generate the subspace \(\Lambda^q(V)\). We now show
  that \(B\) is linearly independent.

  Denote by \(\mathcal I := \{I = (i_j)_{j=1}^q : 1 \leq i_1 < \dots < i_q \leq
  n\}\) the set of strictly increasing \(q\)-tuples. For each \(I := (i_1,
  \dots, i_q) \in \mathcal I\) we define the alternating multilinear map \(\mu_I
  : V^q \to k\) as
  \[
    \mu_I(v_1, \dots, v_q) = \sum_{\sigma \in \mathcal S_q}
    \sign(\sigma) \prod_{j=1}^q e_{i_{\sigma(j)}}^*(v_{i_j})
  \] 
  where \(\{e_{i_j}^*\}_{i_j \in I}\) is the dual of \(\{e_{i_j}\}_{i_j
  \in I}\). Using \cref{prop: exterior power universal property} we can
  define a unique linear map \(f_I: \Lambda^q(V) \to k\) such that the diagram
  commutes
  \[
    \begin{tikzcd}
      V^q \ar[r, "\mu_I"] \ar[d, swap, "A \circ \otimes"] &k \\
      \Lambda^q(V) \ar[ur, dashed, swap, "f_I"]
    \end{tikzcd}
  \] 
  which implies in \(\mu_I(v_1, \dots, v_q) = f_I(v_1 \wedge \dots \wedge v_q)\)
  for all \((v_1, \dots, v_q) \in V^q\).

  Let \(I' = (i_1', \dots, i_q') \in \mathcal I\) such that \(I' \neq I\). From
  the strictly ordering of the indices, we conclude that there must exists some
  \(1 \leq j_0 \leq q\) such that \(i_{j_0}' \neq i_j\) for all \(1 \leq j \leq
  q\). In particular, this implies that
  \[
    f_I(e_{i_1'} \wedge \dots \wedge e_{i_q'}) = \mu_I(e_{i_1'}, \dots,
    e_{i_q'}) = \sum_{\sigma \in \mathcal S_q} \sign(\sigma)
    \prod_{j=1}^q e_{i_{\sigma(j)}}^*(e_{i_j'}) = 0
  \] 
  since \(e_{i_{\sigma(j)}}^*(e_{i_{j_0}}) = 0\) for all permutations \(\sigma
  \in \mathcal S_q\) and \(1 \leq j \leq q\). On the other hand, we have that
  \[
    f_I(e_{i_1} \wedge \dots \wedge e_{i_q}) = \mu_I(e_{i_1}, \dots, e_{i_q})
    = \sum_{\sigma \in \mathcal S_q} \sign(\sigma) \prod_{j=1}^q
    e_{i_{\sigma(j)}}^*(e_{i_j})
    = \sign(\Id) \prod_{j=1}^q e_{i_{\Id(j)}}^*(e_{i_j})
    = 1
  \] 

  Let \(c_P \in k\) for all \(P \in \mathcal I\) and consider the vanishing
  linear combination
  \begin{equation}\label{eq: vanishing exterior product combination}
    \sum_{P \in \mathcal I} c_P (e_{p_1} \wedge \dots \wedge e_{p_q}) = 0.
  \end{equation}
  Then, if we look at its image under the map \(f_I\) for all \(I \in \mathcal
  I\) we conclude that
  \[
    0 = f_I\left( \sum_{P \in \mathcal I} c_P (e_{p_1} \wedge \dots \wedge
    e_{p_q}) \right) = \sum_{P \in \mathcal I} c_P f_I(e_{p_1} \wedge \dots \wedge
    e_{p_q}) = c_I 
  \] 
  hence we conclude that the linear combination \cref{eq: vanishing exterior
  product combination} vanishes if and only if each coefficient vanishes. This
  concludes that \(B\) is linearly independent. Hence we've proved that \(B\) is
  a basis for \(\Lambda^q(V)\).
\end{proof}

\begin{proposition}
  Let \(V\) be a \(n\)-dimensional \(k\)-vector space. Then we have
  \[
    \dim_k \Lambda^q(V) = \binom n q
  \] 
\end{proposition}

\begin{proof}
  Since we want our indices to be strictly increasing, we are left with \(n\)
  elements of which we want to arrange in groups of \(q\) elements. Hence the
  number of possible arrangements is \(\binom n q\) and this concludes the
  proof.
\end{proof}

\subsection{Exterior Algebra}

\begin{definition}[Exterior algebra]
  \label{def: exterior algebra}
  Let \(V\) be a \(k\)-vector space. We define the exterior algebra on \(V\) as
  \[
    \Lambda^\bullet(V) = \bigoplus_{q=0}^\infty \Lambda^q(V)
  \] 
  together with a multiplicative structure \(\wedge: \Lambda^d(V) \otimes
  \Lambda^q(V) \to \Lambda^{d+q}(V)\) mapping
  \[
    (v_1 \wedge \dots \wedge v_d) \otimes (w_1 \wedge \dots \wedge w_q) \xmapsto
    \wedge v_1 \wedge \dots \wedge v_d \wedge w_1 \wedge \dots \wedge w_q.
  \] 
  We interpret \(\Lambda^0(V) = k\).
\end{definition}

\begin{proposition}
  The multiplicative structure of the tensor algebra \(\Lambda^\bullet(V)\) is
  associative and skew-commutative, that is, for all \(\alpha \in \Lambda^d(V)\) 
  and \(\beta \in \Lambda^q(V)\) we have \(\alpha \wedge \beta = (-1)^{d q} \beta
  \wedge \alpha\).
\end{proposition}

\begin{proof}
  First, we prove that if \(T_1 \in T_0^q(V)\) and \(T_2 \in T_0^d(V)\) then
  \begin{equation}\label{eq: asso goal}
    A(A(T_1) \otimes T_2) = A(T_1 \otimes A(T_2)) =
    A(T_1 \otimes T_2)
  \end{equation} Notice that \[
    A(T_1) \otimes T_2 = \frac 1 {q!} \sum_{\sigma \in \mathcal S_q}
    \sign(\sigma) f_\sigma(T_1) \otimes T_2
  \] 
  therefore we find
  \begin{equation}\label{eq: A(A(T1) T2)}
    A(A(T_1) \otimes T_2) = \frac 1 {q!} \sum_{\sigma \in \mathcal S_q}
    \sign(\sigma) A(f_\sigma(T_1) \otimes T_2)
  \end{equation}
  Moreover, we can construct an injection of the symmetry groups \(\mathcal S_q
  \mono \mathcal S_{q + d}\) via the mapping \(\sigma \mapsto \overline \sigma\)
  where \(\overline \sigma(i) = \sigma(i)\) for all \(i \in \{1, \dots, q\}\)
  and \(\overline \sigma(i) = i\) for all \(i \in \{q + 1, \dots, q + d\}\). In
  particular, we find that \(f_\sigma(T_1) \otimes T_2 = f_{\overline \sigma}
  (T_1 \otimes T_2)\) and clearly \(\sign(\overline \sigma) = \sign(\sigma)\).
  This implies that 
  \begin{equation}\label{eq: ol sigma and A commute}
    A(f_\sigma(T_1) \otimes T_2) = A(f_{\overline \sigma}(T_1 \otimes T_2))
    = f_{\overline \sigma}(A(T_1 \otimes T_2)) 
    = \sign(\overline \sigma) A(T_1 \otimes T_2)
    = \sign(\sigma) A(T_1 \otimes T_2)
  \end{equation}

  If we substitute \cref{eq: ol sigma and A commute} in \cref{eq: A(A(T1) T2)}
  we find
  \[
    A(A(T_1) \otimes T_2) = \frac 1 {q!} \sum_{\sigma \in \mathcal S_q}
    \sign^2(\sigma) A(T_1 \otimes T_2) = A(T_1 \otimes T_2).
  \] 
  In the same manner we can show that \(A(T_1 \otimes A(T_2)) = A(T_1 \otimes
  T_2)\). Hence \cref{eq: asso goal} holds.

  Let \(\alpha \in \Lambda^d(V), \beta \in \Lambda^q(V), \gamma \in \Lambda^p(V)\).
  Notice that from the construction of the product map \(\wedge\) and \cref{eq:
  asso goal} we find
  \[
    (\alpha \wedge \beta) \wedge \gamma 
    = \wedge\left( \wedge(\alpha \otimes \beta) \otimes \gamma \right) 
    = \wedge\left(\alpha \otimes \wedge(\beta \otimes \gamma) \right)
    = (\alpha \wedge \beta) \wedge \gamma
  \] 
  which proves associativity of the tensor algebra.

  We now prove skew-commutativity. Let \(\alpha \in \Lambda^d(V)\) and \(\beta
  \in \Lambda^q(V)\), and a permutation \(\sigma \in \mathcal S_{q + d}\) such
  that \(\sigma(i) = q + d - i + 1\), consisting of \(d q\) transpositions. 
  Hence we find that 
  \[
    \beta \wedge \alpha = f_\sigma(\alpha \wedge \beta)
    = \sign(\sigma)(\alpha \wedge \beta) = (-1)^{d q} (\alpha \wedge \beta).
  \]
\end{proof}

\begin{proposition}\label{prop: li iff wedge nonzero}
  Let \(V\) be a \(k\)-vector space and \(\{v_i\}_{i=1}^d \subseteq V\). The
  vectors \(v_1, \dots, v_d\) are linearly independent if and only if
  \[
    \Lambda^d(V) \ni v_1 \wedge \dots \wedge v_d \neq 0.
  \] 
\end{proposition}

\begin{proof}
  Let \(\{v_i\}_{i=1}^d\) be a set of linearly dependent vectors and choose
  a set of not-all zero scalars \(\{c_i\}_{i=1}^d \subseteq k\) such that
  \(\sum_{i=1}^d c_i v_i \neq 0\). Choose \(1 \leq j \leq d\) such that \(c_j
  \neq 0\) and write \(v_j = -\sum_{i=1}^d \frac{c_i}{c_j} v_i\). Then we find
  that (recall \(\mathcal M_0\) from \cref{def: subspace M_0})
  \begin{align}
    \nonumber
    v_1 \wedge \dots \wedge v_j \wedge \dots \wedge v_d
    &= v_1 \wedge \dots \wedge \left( - \sum_{i=1}^d \frac{c_i}{c_j} v_i \right)
    \wedge \dots \wedge v_d 
    \\
    \nonumber
    &= \left( v_1, \dots, - \sum_{i=1}^d \frac{c_i}{c_j} v_i, \dots, v_d \right)
    + \mathcal M_0 \\
    \label{eq: split class M_0 wedge}
    &= -\sum_{i=1}^d \frac{c_i}{c_j} (v_1, \dots, v_i, \dots, v_d) + \mathcal
    M_0
    \\
    \label{eq: wedge repeated v_i}
    &= - \sum_{i=1}^d \frac{c_i}{c_j} (v_1 \wedge \dots \wedge v_i \wedge \dots
    v_d)
    \\
    \label{eq: wedge zero}
    &= 0
  \end{align}
  where \cref{eq: split class M_0 wedge} comes from the fact that \(a(v_1, \dots,
  v_{j-1}, 0, v_{j+1}, \dots, v_d) \in \mathcal M_0\) and therefore we have
  \begin{align*}
    (v_1, \dots, a v_j + b w, \dots, v_d) + \mathcal M_0
    &= (v_1, \dots, a v_j, \dots, v_d) + (v_1, \dots, b w, \dots, v_d) 
    + \mathcal M_0 \\
    &= a(v_1, \dots, v_j, \dots, v_d) + b(v_1, \dots, w, \dots, v_d) 
    + \mathcal M_0.
  \end{align*}
  On the other hand, \cref{eq: wedge zero} comes from the fact that we have a
  repeated \(v_i\) in the wedge product \cref{eq: wedge repeated v_i}.

  Suppose \(\{v_i\}_{i=1}^d\) is a linearly independent set. Then from
  \cref{prop: li to basis} we can build \(B \supseteq \{v_i\}_{i=1}^d\) such
  that \(B\) is a basis for \(V\). From \cref{prop: exterior power basis}, we
  find that \(\mathcal B = \{v_{i_1} \wedge \dots v_{i_d} : v_{i_j} \in B \text{
  and } i_1 < \dots < i_d\}\) is a basis for \(\Lambda^d(V)\). In particular,
  \(v_1 \wedge \dots \wedge v_d \in \mathcal B\), hence necessarily \(v_1 \wedge
  \dots \wedge v_d \neq 0\).
\end{proof}

\todo[inline]{
Add cool propositions for the exterior algebra: the Hodge Star Operator; the
isomorphism \(\Lambda^d(V^*) \iso (\Lambda^d(V))^*\); and the natural
isomorphisms
\begin{gather*}
  \Sym^d(V \oplus W) \iso \bigoplus_{i=0}^d \Sym^i(V) \otimes
  \Sym^{d-i}(W) \\
  \Lambda^d(V \oplus W) \iso \bigoplus_{i=0}^d \Lambda^i(V) \otimes
  \Lambda^{d-i}(W)
\end{gather*}
}

\todo[inline]{After creating sections on affine and projective geormetry,
introduce grassmanian varieties.}
