\documentclass[11pt, reqno]{amsart}
\usepackage[english]{babel}

\usepackage{layout}
\usepackage{afterpage}
\usepackage[
  asymmetric,
  textheight     = 673pt,
  marginparsep   = 7pt,
  footskip       = 27pt,
  hoffset        = 0pt,
  paperwidth     = 597pt,
  textwidth      = 452pt,
  marginparwidth = 101pt,
  voffset        = 0pt,
  paperheight    = 845pt,
]{geometry}

\newcommand{\changegeometry}{\newgeometry{includehead,headheight=89pt}%
  \afterpage{\aftergroup\restoregeometry}%
}

% Math stuff: do not mess with the ordering!
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{tikz-cd}
\usepackage{tqft}

% Font
% \usepackage[no-math]{newpxtext}
% \usepackage{newpxmath}
% % Set arrow tip to that of newpxmath
% \tikzset{>=Straight Barb, commutative diagrams/arrow style=tikz}

% Utilities
\usepackage{enumerate}
\usepackage{todonotes}
\usepackage{graphicx}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Horizontal rule
\usepackage{multirow}

% Color
\usepackage{xcolor}
\definecolor{brightmaroon}{rgb}{0.76, 0.13, 0.28}

\usepackage{pdfpages}

% References
\usepackage{hyperref}
\hypersetup{
  colorlinks = true,
  allcolors  = brightmaroon,
}
\usepackage[capitalize,nameinlink]{cleveref}
\usepackage[
  backend = biber,
  style   = alphabetic,
]{biblatex}
\addbibresource{bibliography.bib}


% Table of contents: show subsections
\setcounter{tocdepth}{2}

\linespread{1.05}
\vfuzz=14pt % No more vbox errors all over the place

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** Environments **

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{notation}[theorem]{Notation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** symbols **

\renewcommand{\qedsymbol}{\(\natural\)}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\setminus}{\smallsetminus}
\renewcommand{\preceq}{\preccurlyeq}

% ':' for maps and '\colon' for relations on collections
\DeclareMathSymbol{:}{\mathpunct}{operators}{"3A}
\let\colon\relax
\DeclareMathSymbol{\colon}{\mathrel}{operators}{"3A}

% Disjoint unions over sets
\newcommand{\disj}{\amalg}     % Disjoint union
\newcommand{\bigdisj}{\coprod} % Indexed disjoint union

\DeclareMathOperator{\Log}{Log}
\newcommand{\img}{\text{i}}

% Constant map
\DeclareMathOperator{\const}{cons}

% Multiplication map
\DeclareMathOperator{\mul}{mul}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** arrows **

% Alias for Rightarrow
\newcommand{\To}{\Rightarrow}

% Monomorphism arrow
\newcommand{\mono}{\rightarrowtail}

% Epimorphism arrow
\newcommand{\epi}{\twoheadrightarrow}

% Unique morphism
\newcommand{\unique}{\to}%{\dashrightarrow}
\newcommand{\xdashrightarrow}[2][]{\ext@arrow 0359\rightarrowfill@@{#1}{#2}}

% Isomorphism symbol
\newcommand{\iso}{\simeq}

\newcommand{\arrowiso}{\iso}
% Isomorphism arrow
\newcommand{\isoto}{\xrightarrow{\raisebox{-.6ex}[0ex][0ex]{\(\arrowiso\)}}}

% How isomorphisms are depicted in diagrams: either \sim or \simeq
\newcommand{\dis}{\iso}

\newcommand{\isounique}{%
  \xdashrightarrow{\raisebox{-.6ex}[0ex][0ex]{\(\arrowiso\)}}
}%

% Natural transformation arrow
\newcommand{\nat}{\Rightarrow}

% Natural isomorphism
\newcommand{\isonat}{\xRightarrow{\raisebox{-.6ex}[0ex][0ex]{\(\arrowiso\)}}}

% Embedding arrow
\newcommand{\emb}{\hookrightarrow}

% Parallel arrows
\newcommand{\para}{\rightrightarrows}

% Adjoint arrows
\newcommand{\adj}{\rightleftarrows}

% Implication
\renewcommand{\implies}{\Rightarrow}
\renewcommand{\impliedby}{\Leftarrow}

% Limits
\DeclareMathOperator{\Lim}{lim}     % Limit
\DeclareMathOperator{\Colim}{colim} % Colimit
\DeclareMathOperator{\Eq}{eq}       % Equalizer
\DeclareMathOperator{\Coeq}{coeq}   % Coequalizer

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** Common collections **

\newcommand{\Z}{\mathbf{Z}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\CC}{\mathbf{C}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\FF}{\mathbf{F}}

\renewcommand{\emptyset}{\varnothing}

\newcommand{\Uhs}{\mathbf{H}}  % Upper half space
\newcommand{\Proj}{\mathbf{P}} % Projective space

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** Categories **

% Font for categories
\newcommand{\cat}{\texttt}
\newcommand{\catfont}{\texttt}

% Opposite category
\newcommand{\op}{\mathrm{op}}

% Common categories
\newcommand{\Set}{{\catfont{Set}}}          % Sets
\newcommand{\FinSet}{{\catfont{FinSet}}}    % Finite sets
\newcommand{\pSet}{{\catfont{pSet}}}        % Pointed sets
\newcommand{\tOrd}{{\catfont{tOrd}}}        % Totally ordered sets

\newcommand{\Vect}{{\catfont{Vect}}}        % Vector spaces
\newcommand{\FinVect}{{\catfont{FinVect}}}  % Finite vector spaces

\newcommand{\TVect}{{\catfont{TVect}}}      % Topological vector spaces
\newcommand{\Ban}{{\catfont{Ban}}}          % Banach spaces

\newcommand{\Grp}{{\catfont{Grp}}}          % Groups
\newcommand{\Ab}{{\catfont{Ab}}}            % Abelian groups
\newcommand{\GSet}[1]{{{#1}\text{-}\Set}}   % G-sets
\newcommand{\Grpd}{{\catfont{Grpd}}}        % Groupoids

\newcommand{\Mon}{{\catfont{Mon}}}          % Monoidal category
\newcommand{\coMon}{{\catfont{coMon}}}      % coMonoidal category
\newcommand{\rActMon}{{\catfont{rActMon}}}  % right action category
\newcommand{\lActMon}{{\catfont{lActMon}}}  % left action category
\newcommand{\BrMonCat}{{\catfont{BrMonCat}}} % Cat of braided monoidal cats

\newcommand{\Graph}{{\catfont{Graph}}}      % Graphs
\newcommand{\SimpGraph}{{\catfont{sGraph}}} % Simple graphs
\newcommand{\ProfCol}{{\catfont{Prof}(\Col)}}   % C-profile category


\newcommand{\Rng}{{\catfont{Ring}}}             % Rings
\newcommand{\cRng}{{\catfont{CRing}}}           % Commutative rings
\newcommand{\rMod}[1]{{\texttt{Mod}_{#1}}}      % Right modules
\newcommand{\lMod}[1]{{{}_{#1}\catfont{Mod}}}   % Left modules
\newcommand{\Mod}[1]{{#1\text{-}\catfont{Mod}}} % Modules over comm. ring
\newcommand{\Alg}[1]{{#1\text{-}\catfont{Alg}}} % Algebras
\newcommand{\cAlg}[1]{{#1\text{-}\catfont{CAlg}}} % Commutative algebras

\newcommand{\Cat}{{\catfont{Cat}}}          % Small categories
\newcommand{\CAT}{{\catfont{CAT}}}          % Big categories
\newcommand{\UCat}{{\mathcal{U}\text{-}\catfont{Cat}}} % U-Categories

\newcommand{\Psh}[1]{{\catfont{Psh}({#1})}}   % Category of presheaves
\newcommand{\comma}{\downarrow} % Comma category separator
\DeclareMathOperator{\El}{El}              % Category of elements

\DeclareMathOperator{\Map}{Map}

% Operators
\DeclareMathOperator{\Hom}{Mor}   % Morphisms
\DeclareMathOperator{\Mono}{Mono} % Monomorphisms
\DeclareMathOperator{\Epi}{Epi}   % Epimorphisms
\DeclareMathOperator{\Fct}{Fct}   % Functors
\DeclareMathOperator{\Obj}{Obj}   % Objects
\DeclareMathOperator{\Mor}{Mor}   % Morphisms, again
\DeclareMathOperator{\End}{End}   % Endomorphisms
\DeclareMathOperator{\Aut}{Aut}   % Automorphisms
\DeclareMathOperator{\Id}{id}     % Identity
\DeclareMathOperator{\im}{im}     % Image
\DeclareMathOperator{\dom}{dom}   % Domain
\DeclareMathOperator{\codom}{cod} % Codomain
\DeclareMathOperator{\supp}{supp} % Support

% Yoneda embedding
\newcommand{\yo}{\text{\usefont{U}{min}{m}{n}\symbol{'210}}}
\DeclareFontFamily{U}{min}{}
\DeclareFontShape{U}{min}{m}{n}{<-> udmj30}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** algebra **
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\Tr}{tr}   % Trace
\DeclareMathOperator{\Sym}{Sym} % Symmetric space
\DeclareMathOperator{\Alt}{Alt} % Alternating space
\DeclareMathOperator{\Ann}{Ann} % Annihilator
\DeclareMathOperator{\Char}{char}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Inn}{Inn}     % Inner automorphisms
\DeclareMathOperator{\Spec}{Spec}   % Prime spectrum
\DeclareMathOperator{\Specm}{Spec_m} % Maximal spectrum
\newcommand{\lie}[1]{\mathfrak{#1}} % Font for Lie structures
\DeclareMathOperator{\Rees}{Rees}   % Rees algebra
\DeclareMathOperator{\Frac}{Frac}   % Field of fractions

\DeclareMathOperator{\torsion}{\texttt{tor}}   % Torsion

\DeclareMathOperator{\eval}{eval}
\DeclareMathOperator{\sign}{sign}

% Matrices
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\PSL}{PSL}
\DeclareMathOperator{\SO}{SO}
\DeclareMathOperator{\SU}{SU}
\DeclareMathOperator{\Unit}{U}
\DeclareMathOperator{\Orth}{O}

% Symbol for the group of units --- for instance, the group of units of a ring
% \(R\) will be denoted by \(R^{\unit}\).
\newcommand{\unit}{\times}

% Orbit and stabilizer
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}

% Left and right group actions
\newcommand{\laction}{\circlearrowright}
\newcommand{\raction}{\circlearrowleft}

% Ring ideals font
\newcommand{\ideal}[1]{\mathfrak{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** Topology **

\let\Top\relax
\newcommand{\Top}{{\catfont{Top}}}                       % Topological spaces

\newcommand{\wHTop}{{\catfont{wH}\text{-}\catfont{Top}}} % Weak Hausdorff
\newcommand{\kTop}{{k\text{-}\catfont{Top}}}             % k-spaces
\newcommand{\cgTop}{{\catfont{cgTop}}} % Compactly generated

\newcommand{\pTop}{{\catfont{Top}^{*}}}   % Pointed top spaces
\newcommand{\bpTop}{{\catfont{Top}^{*/}}} % Base point top spaces

\newcommand{\Ho}[1]{{\catfont{Ho}(#1)}}            % Homotopy category
\newcommand{\HoTop}{{\catfont{Ho}(\catfont{Top})}} % Classical Homotopy cat

\newcommand{\Splx}{{\mathbf{\Delta}}}           % Simplex category
\newcommand{\sSet}{{\catfont{sSet}}}            % Simplicial sets
\newcommand{\Simp}[1]{{\catfont{Simp}(#1)}}     % Simplicial category
\newcommand{\CoSimp}[1]{{\catfont{CoSimp}(#1)}} % Cosimplicial category

\newcommand{\splx}{\Delta}
\newcommand{\splxtop}{\Delta_{\text{top}}}

\DeclareMathOperator{\Sing}{Sing} % Singular complex functor
\DeclareMathOperator{\Nondeg}{nd} % Non-degenerate simplices
\newcommand{\disc}{\text{disc}}   % discrete

\DeclareMathOperator{\Sk}{sk} % Skeleton

\DeclareMathOperator{\Cof}{Cof} % Cofibrant replacement
\DeclareMathOperator{\Fib}{Fib} % Fibrant replacement

% attaching spaces
\newcommand{\att}{\amalg}     % Disjoint union
\newcommand{\bigatt}{\coprod} % Indexed disjoint union

% Homotopy
\newcommand{\simht}{\sim_{\text{h}}}              % Homotopy between maps
\newcommand{\simhtrel}[1]{\sim_{\text{rel }{#1}}} % Relative htpy
\newcommand{\isoht}{\iso_{\text{h}}}              % Homotopy equivalence
\newcommand{\htpy}{\Rightarrow}                   % Htpy arrow
\newcommand{\htpyrel}[1]{\Rightarrow_{\text{rel }{#1}}} % Relative htpy arrow


% Functors on topological spaces
\DeclareMathOperator{\Disc}{Disc}       % Discrete topology: Set -> Top
\DeclareMathOperator{\Cone}{Cone}       % Cone
\DeclareMathOperator{\Cyl}{Cyl}         % Cylinder
\DeclareMathOperator{\Susp}{S}          % Suspension
\DeclareMathOperator{\rSusp}{\Sigma}    % Reduced suspension
\DeclareMathOperator{\Path}{Path}       % Path object
\DeclareMathOperator{\Loop}{\Omega}       % Loop space
\DeclareMathOperator{\Eval}{eval}       % Evaluation map
\DeclareMathOperator{\curry}{curry}     % Currying a map
\DeclareMathOperator{\uncurry}{uncurry} % Currying a map
\DeclareMathOperator{\co}{co}           % Compact open co(K, U)

\newcommand{\lift}{\widehat} % Lifting of path and stuff

% Set operators
\DeclareMathOperator{\Cl}{Cl}       % Closure
\DeclareMathOperator{\Bd}{\partial} % Boundary
\DeclareMathOperator{\Int}{Int}     % Interior
\DeclareMathOperator{\Ext}{Ext}     % Exterior

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** Differentiable structures **

\newcommand{\Bun}{{\catfont{Bun}}} % Bundle category
\newcommand{\VecBun}{{\catfont{VecBun}}} % Vector Bundle category
\newcommand{\Man}{{\catfont{Man}}} % Manifolds

% Norm
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

% Differential operators
\newcommand{\diff}{\mathrm{d}}
\newcommand{\Diff}{\mathrm{D}}
\DeclareMathOperator{\grad}{grad} % Gradient
\DeclareMathOperator{\Hess}{Hess} % Hessian
\DeclareMathOperator{\Jac}{Jac}   % Jacobian
\DeclareMathOperator{\Curl}{Curl} % Curl
\DeclareMathOperator{\VecField}{\mathfrak{X}}

\DeclareMathOperator{\Grass}{Grass}    % Grassmann variety
\DeclareMathOperator{\Stie}{Stie}      % Stiefel variety

% Vector bundle
\DeclareMathOperator{\zerosec}{Zero} % Zero section

\newcommand{\trans}{\pitchfork} % Transversality

% Set operators
\DeclareMathOperator{\Vol}{vol}   % Volume
\DeclareMathOperator{\Mesh}{mesh} % Mesh

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** Graphs **

% Colouring
\newcommand{\Col}{\mathfrak{C}}
\newcommand{\prof}[1]{\underline{#1}}

\DeclareMathOperator{\Edge}{Edge}
\DeclareMathOperator{\Vertex}{Vert}
\DeclareMathOperator{\Circ}{circ}
\DeclareMathOperator{\diam}{diam}
\newcommand{\emptygraph}{\varnothing}
\DeclarePairedDelimiterX{\size}[1]{\lVert}{\rVert}{#1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** machine learning **

\DeclareMathOperator{\Prob}{\mathbb{P}} % Probability
\DeclareMathOperator{\Expect}{\mathbb{E}} % Expectation
\newcommand{\Mean}{\mu} % Mean
\DeclareMathOperator{\Cov}{Cov} % Covariance
\DeclareMathOperator{\Var}{Var} % Variance
\DeclareMathOperator{\Correlation}{Corr}
\DeclareMathOperator{\stdev}{\sigma} % standard deviation
\DeclareMathOperator{\ndist}{\mathcal{N}} % Normal distribution
\DeclareMathOperator{\precision}{\beta}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\vcdim}{VCdim}
\DeclareMathOperator{\Growth}{Growth} % Growth function
\DeclareMathOperator{\Quantile}{Quant}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ** MACROS END HERE **
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\includepdf[pages=-]{anexo_II_encaminhamento_de_relatorio.pdf}

\begin{titlepage}
 \vfill
  \begin{center}
       \textsc{\LARGE \textbf{University of São Paulo}} \\[2.0cm]

       \vskip 0.5cm
       \textsc{\large Luiz Gustavo Mugnaini Anselmo}

       {\normalsize Molecular Sciences Course \\
         Class 30, n\(^{\text{o}}\)
         USP 11809746

       E-mail: \texttt{luizmugnaini@usp.br}}\\[2.0cm]

       \HRule\\
       \vskip 0.5cm
       {\LARGE \textbf{Dendroidal Homotopy Theory \& Operads}}
       \HRule\\[1.5cm]

       \hspace{.45\textwidth}
       \begin{minipage}{.5\textwidth}
       \normalsize \textbf{Advanced Studies Report II}\\[0.5cm]

       \textsc{\large Prof.~Ivan Struchiner}\\
       University of São Paulo \\
       Institute of Mathematics and Statistics \\
       E-mail: \texttt{ivanstru@ime.usp.br}\\[1cm]

       \normalsize São Paulo, July of 2023
       \end{minipage}
  \end{center}
\end{titlepage}

% Title stuff
\title[Dendroidal Homotopy Theory \& Operads]{%
{\footnotesize\sl Advanced Studies Report II} \\ \smallskip
  Dendroidal Homotopy Theory \& Operads
}%

\author{%
  Luiz Gustavo Mugnaini Anselmo and Ivan Struchiner
}%

\address{%
  Institute of Mathematics and Statistics, University of São
  Paulo, Rua do Matão 1010, 05508--090~São Paulo, SP
}%

\email{luizmugnaini@usp.br, ivanstru@ime.usp.br}

\begin{abstract}
This report documents the research advances of the student Luiz G. Mugnaini
A. during his second semester of the advanced cycle of the Molecular Sciences
course. The main topic of research explored this semester was the study of model
categories, algebraic topology, homological algebra and machine learning.
\end{abstract}
\maketitle

\section{This Semester \&  the Future of the Project}

With the insurgence of geometric and topological deep learning, we think it
would be a great opportunity to utilise our knowledge of algebraic topology to
help advance these fields of research---which currently lack mathematical
formality and a good framework to build upon. This led us to the decision that
part of our research group would start focusing on topological deep learning
using techniques from pure mathematics. With this in mind, we opted to add to my
semester (2023/1) a course on machine learning (MAC5832). Moreover, we thought
it would be a great idea to exchange the course on algebraic geometry (MAT5761)
for one in homological algebra (MAT5997), which would best fit our intentions
with the current research. All alterations where properly approved by Ivan
Struchiner.

\begin{table}[h!]\label{tab:disc}
  \centering
  \caption{Modified list of courses attended in the first semester of 2023.}
  \begin{tabular}{ |c|c|c|c|c| }
    \hline
    Year and Semester & Code & Discipline & Type & Credits \\
    \hline
    \multirow{4}{*}{2023 / 1}
                   & CCM0328 & Iniciação à Pesquisa II            & G  & 12 \\
                   & MAT5997 & Tópicos de Álgebra   & PG & 8 \\
                   & MAT6684 & Topologia Algébrica                & PG & 8 \\
                   & MAC5832 & Introdução ao Aprendizado de Máquina & PG & 8 \\
    \hline
\end{tabular}
\end{table}

\section{Model Categories}

Given a category \(\cat C\) we define the category \(\Map \cat C\) to be the
category whose objects are morphisms of \(\cat C\) and objects are commutative
diagrams in \(\cat C\).

\begin{definition}
\label{def:retracts-and-functorial-factorisations}
Let \(\cat C\) be a category. We define the following:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item A morphism \(f \in \Mor \cat C\) is a \emph{retract} of a morphism
  \(g \in \Mor \cat C\) if and only if there exists a commutative diagram of the
  form
  \[
  \begin{tikzcd}
  A \ar[r] \ar[d, "f"'] \ar[rr, "\Id_A", bend left]
  &C \ar[r] \ar[d, "g"]
  &A \ar[d, "f"] \\
  B \ar[r] \ar[rr, "\Id_B"', bend right]
  &D \ar[r]
  &B
  \end{tikzcd}
  \]
  that is, \(f\) is a retract of \(g\) as objects of \(\Map \cat C\).

\item A \emph{functorial factorisation} is an ordered pair of
  functors \(\alpha, \beta: \Map \cat C \to \Map \cat C\) for which \(f = (\beta f)(\alpha f)\) for
  any \(f \in \Map \cat C\).
\end{enumerate}
\end{definition}

\begin{definition}[Lifting properties]
\label{def:lifting-properties}
Let \(\cat C\) be a category and consider morphisms \(i: A \to B\) and \(p: X \to
Y\) in \(\cat C\). We say that \(i\) has the \emph{left lifting property} with
respect to \(p\)---and thus \(p\) has the \emph{right lifting property} with
respect to \(i\)---if the for every commutative diagram of the form:
\[
\begin{tikzcd}
A \ar[r, "f"] \ar[d, "i"'] &X \ar[d, "p"] \\
B \ar[r, "g"'] &Y
\end{tikzcd}
\]
there exists a morphism \(h: B \to X\) in \(\cat C\) for which the following
diagram commutes:
\[
\begin{tikzcd}
A \ar[r, "f"] \ar[d, "i"'] &X \ar[d, "p"] \\
B \ar[r, "g"'] \ar[ru, "h"] &Y
\end{tikzcd}
\]
\end{definition}

\begin{definition}[Model structure]
\label{def:model-structure}
Let \(\cat C\) be a category. We define a \emph{model structure} on \(\cat C\)
to be a triple of subcategories of \(\cat C\) called \emph{weak equivalences},
\emph{cofibrations}, and \emph{fibrations}, together with two functorial
factorisations \((\alpha, \beta)\) and \((\gamma, \delta)\) such that the following properties are
satisfied:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item (Two out of three) Let \(f\) and \(g\) be composable morphisms of
  \(\cat C\). If any two of the morphisms \((f, g, g f)\) are weak
  equivalences, then so is the remaining third.

\item (Retracts) Let \(f, g \in \Mor \cat C\). If \(f\) is a retract of \(g\) and
  \(g\) is a weak equivalence, cofibration, or fibration, then so is \(f\). That
  is, these subcategories are closed under retracts.

\item (Lifting) A morphism is said to be a \emph{trivial (co)fibration} if it is
  both a (co)fibration and a weak equivalence. Then the following is true:
  \begin{itemize}\setlength\itemsep{0em}
  \item Trivial cofibrations have the left lifting property with respect to
    fibrations.
  \item Trivial fibrations have the right lifting property with respect to
    cofibrations.
  \end{itemize}

\item (Factorisation) Consider any \(f \in \Mor \cat C\). Then \(\alpha f\) is a
  cofibration, \(\beta f\) is a trivial fibration, \(\gamma f\) is a trivial cofibration,
  and \(\delta f\) is a fibration.
\end{enumerate}
\end{definition}

\begin{definition}[Model category]
\label{def:model-category}
A \emph{model category} is a complete and cocomplete category together with a
model structure.
\end{definition}

Since cocomplete categories admit an initial object (the colimit of the empty
diagram) complete categories admit terminal object (the limit of the empty
diagram), it follows that every model category admits both an initial and a
terminal object. We define the following kinds of objects of a model category:

\begin{definition}[(Co)fibrant objects]
\label{def:(co)fibrant-objects}
Let \(\cat C\) be a model category, with initial object \(0\) and terminal
object \(1\). We define:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item An object \(C \in \cat C\) is said to be \emph{cofibrant} if the unique map
  \(0 \to C\) is a cofibration.

\item An object \(F \in \cat C\) is said to be \emph{fibrant} if the unique map
  \(F \to 1\) is a fibration.
\end{enumerate}
\end{definition}

Furthermore, for every category \(\cat C\) admitting an initial and terminal
object, we say that \(\cat C\) is a \emph{pointed category} if the map \(0 \to 1\)
is an isomorphism.

Let \(\cat C\) be a model category with a terminal object denoted by \(*\). We
define \(\cat C_{/*}\) to be the slice category under \(*\), that is, composed of
pairs \((X, v)\) where \(X \in \cat C\) and \(v \in \Mor_{\cat C}(*, X)\)---where
\(v\) is said to be the \emph{basepoint} of \(X\). A morphism between objects of
\(\cat C_{/*}\) should preserve basepoints.

\begin{proposition}
\label{prop:slice-over-terminal-is-model-cat}
Let \(\cat C\) be a model category with terminal object \(*\). Let
\(U: \cat C_{/*} \to \cat C\) denote the canonical forgetful functor. Given a
morphism \(f \in \Mor \cat C_{/*}\), then \(f\) is a cofibration (respectively, a
fibration or weak equivalence) if and only if \(U f\) is a cofibration
(respectively, fibration or weak equivalence). Therefore \(\cat C_{/*}\) is a
model category.
\end{proposition}

Let \((X, v)\) be an object of the slice category of \(\cat C\) over the initial
object \(0\), denoted \(\cat C_{0/}\). Then the functor \(\beta\) induces a functor
\(\Cof: \cat C_{0/} \to \cat C\) for which \(\Cof X\) is a cofibrant object of
\(\cat C\). This functor \(\Cof\) is called the \emph{cofibrant replacement
  functor} of \(\cat C\). Analogously, if we consider the category
\(\cat C_{/*}\) together with the functor \(\alpha\), we see that it induces a
functor \(\Fib: \cat C_{/*} \to \cat C\) which maps objects of \(\cat C\) to their
corresponding fibrant objects according to \(\alpha\). The functor \(\Fib\) is called
the \emph{fibrant replacement functor} of \(\cat C\).

\begin{lemma}[Retract argument]
\label{lem:retract-argument}
Let \(\cat C\) be a model category and consider a factorisation \(f = p i\) in
\(\cat C\). The following are properties of the factorisation:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item If \(f\) has the left lifting property with respect to \(p\), then \(f\)
  is a retract of \(i\).

\item If \(f\) has the right lifting property with respect to \(i\), then \(f\)
  is a retract of \(p\).
\end{enumerate}
\end{lemma}

The following is an important consequence of the retract argument:

\begin{lemma}[Determining (co)fibrations]
\label{lem:determining-cofibrations-and-fibrations}
Let \(\cat C\) be a model category. Then the following are properties of
the morphisms of \(\cat C\):
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item A morphism is a cofibration (respectively, trivial cofibration) if and
  only if it has the left lifting property with respect to all trivial
  fibrations (respectively, fibrations).

\item A morphism is a fibration (respectively, trivial fibration) if and
  only if it has the right lifting property with respect to all trivial
  cofibrations (respectively, cofibrations).
\end{enumerate}
This shows that \(\cat C\) could be determined using only one of the
subcategories \(\Cof \cat C\) or \(\Fib \cat C\), together with the subcategory of
weak equivalences of \(\cat C\).
\end{lemma}

\begin{lemma}[Ken Brown]
\label{lem:ken-brown}
Let \(\cat C\) be a model category and \(\cat D\) be a category together with a
subcategory of weak equivalences satisfying the two-out-of-three
axiom. Considering a functor \(F: \cat C \to \cat D\), we have:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item If \(F\) takes all trivial cofibrations between cofibrant objects of
  \(\cat C\) to weak equivalences of \(\cat D\), then \(F\) takes all weak
  equivalences between cofibrant objects of \(\cat C\) to weak equivalences of
  \(\cat D\).

\item If \(F\) takes all trivial fibrations between fibrant objects of
  \(\cat C\) to weak equivalences of \(\cat D\), then \(F\) takes all weak
  equivalences between fibrant objects of \(\cat C\) to weak equivalences of
  \(\cat D\).
\end{enumerate}
\end{lemma}

\begin{definition}[Homotopy category]
\label{def:homotopy-category}
Let \(\cat C\) be a model category with a subcategory \(\cat W\) of weak
equivalences. In order to define the \emph{homotopy \(2\)-category}
\(\Ho{\cat C}\), we first need the following construction:

Let \(\cat C[\cat W^{-1}]\) be the free category of arrows of \(\cat C\)
together with inverted arrows of \(\cat W\). The objects of
\(\cat C[\cat W^{-1}]\) are the objects of \(\cat C\), while a morphism is a
string of composable arrows \((f_1, \dots, f_n)\), where
\(f_j \in \Mor(\cat C) \cup \Mor(\cat W^{-1})\)---the empty string is simply the
identity at an object. Composition of morphisms in \(\cat C[\cat W^{-1}]\) are
concatenations of strings.

We define \(\Ho{\cat C}\) to be the category whose objects are those of \(\cat
C\) and
\[
\Mor(\Ho{\cat C}) = \Mor(\cat C[\cat W^{-1}])/{\sim},
\]
where \(\sim\) is an equivalence relation described as follows:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item For any \(X \in \cat C\) we have \(\Id_X \sim (\Id_X)\).

\item For any composable morphisms \(f, g \in \Mor \cat C\) we have \(g f \sim (f, g)\)

\item For any weak equivalence \(w \in \Mor \cat W\) we have \(\Id_{\dom w} \sim (w,
  w^{-1})\) and \(\Id_{\codom w} \sim (w^{-1}, w)\).
\end{enumerate}
\end{definition}


\section{Mathematical Foundations of Machine Learning}

\begin{definition}[True error]
\label{def:true-error}
Consider a target function \(f: \mathcal{X} \to \mathcal{Y}\) and a probability
distribution \(\mathcal{D}\) over \(\mathcal{X}\). If
\(h: \mathcal{X} \to \mathcal{Y}\) is a prediction rule for the learning problem
\((\mathcal{D}, f)\), we define the \emph{true error} of \(h\) as
\[
L_{(\mathcal{D}, f)}(h)
\coloneq \Prob_{x \sim \mathcal{D}}(h(x) \neq f(x))
= \mathcal{D}(\{x \in \mathcal{X} \colon h(x) \neq f(x)\}),
\]
which is the probability of sampling a point \(x \in \mathcal{X}\), according to
the distribution \(\mathcal{D}\), on which the predictor \(h\) fails to meet \(f\).
\end{definition}

\begin{definition}[Training error]
\label{def:training-error}
Let \(S = (x_i, y_i)_{i=1}^m\) be a training set, sampled from an unknown
probability distribution \(\mathcal{D}\) and labelled by a target function
\(f: \mathcal{X} \to \mathcal{Y}\). Let \(h_S: \mathcal{X} \to \mathcal{Y}\) be the hypothesis returned by a learning
algorithm based on the training set \(S\). We define the \emph{training error}
\(L_S: \mathcal{Y}^{\mathcal{X}} \to \R\) to be the map
\[
L_S(h) \coloneq \frac{|\{i \in [m] \colon h(x_i) \neq y_i\}|}{m}.
\]
\end{definition}

\begin{definition}[Empirical risk minimisation]
\label{def:empirical-risk-minimisation}
The learning paradigm known as \emph{empirical risk minimisation}
(ERM) is defined to have the goal of minimising the training
error. As this may cause overfitting, we restrict the set of possible predictors
to a hypothesis collection \(\mathcal{H}\)---this approach induces a bias to the
model. Given a training sample \(S\), we define the learner
\(\text{ERM}_{\mathcal{H}}: 2^{\mathcal{X}} \to \mathcal{Y}^{\mathcal{X}}\) to
choose a predictor \(\text{ERM}_{\mathcal{H}}(S) \coloneq h_S\) such that
\[
h_S \in \argmin_{h \in \mathcal{H}} L_S(h).
\]
\end{definition}

\begin{definition}[Realisability assumption]
\label{def:realisability-assumption}
Within the hypothesis collection \(\mathcal{H}\) there exists \(h^{\star}\) for
which the training error is null:
\[
L_S(h^{\star}) = 0.
\]
\end{definition}

\begin{definition}[Independently identically distributed assumption (i.i.d.)]
\label{def:iid-assumption}
We shall assume that the training set \(S\) has points which are independently
and identically distributed (i.i.d.) with respect to the probability distribution
\(\mathcal{D}\), shortly we write that \(S \sim \mathcal{D}^{|S|}\).
\end{definition}

Since \(S\) is chosen by a randomised procedure, the predictor \(h_S\) and risk
\(L_{(\mathcal{D}, f)}(h_S)\) are both random variables. This allows us to talk
about the probability of \(L_{(\mathcal{D}, f)}(h_S)\) not being large.

\begin{definition}[Confidence parameter]
\label{def:confidence-parameter}
We shall denote by \(\delta\) the probability of \(S\) being a
\emph{non-representative} sample of \(\mathcal{X}\) with respect to the
distribution \(\mathcal{D}\), and \(1 - \delta\) the \emph{confidence parameter}
of the predictor \(h_S\).
\end{definition}

\begin{definition}[Accuracy parameter]
\label{def:accuracy-parameter}
We denote by \(\varepsilon\) the \emph{accuracy parameter}, which measures how
large is \(L_{(\mathcal{D}, f)}(h_S)\) as follows: if
\(L_{(\mathcal{D}, f)}(h_S) > \varepsilon\) we interpret the result of the
algorithm as a \emph{failure}, while
\(L_{(\mathcal{D}, f)}(h_S) \leq \varepsilon\) as a \emph{approximately correct}
predictor.
\end{definition}

\begin{proposition}[Failure bound]
\label{prop:finite-hypothesis-class-failure-bound}
Consider a learner with a finite hypothesis class \(\mathcal{H}\). The
probability of obtaining a sample \(S\), of size \(m\), on which the learner
fails the accuracy parameter is bound by
\[
\mathcal{D}^m(\{\pi_{\mathcal{X}} S \colon L_{(\mathcal{D}, f)}(h_S) > \varepsilon\})
\leq |\mathcal{H}| e^{-\varepsilon m}
\]
where \(\pi_{\mathcal{X}}\) is the projection of the first coordinate.
\end{proposition}

\begin{corollary}
\label{cor:finite-hypothesis-pac-learnability}
Let \(\mathcal{H}\) be a finite hypothesis collection, and consider parameters
\(0 < \delta < 1\) and \(\varepsilon > 0\). If \(m \in \N\) is such that
\[
m \geq \frac{\log(|\mathcal{H}| / \delta)}{\varepsilon},
\]
then for any labelling map \(f: \mathcal{X} \to \mathcal{Y}\) together with a
distribution \(\mathcal{D}\) following the realisability assumption, we have a
probability of at least \(1 - \delta\) of choosing an i.i.d.~sample \(S\) with
size \(m\) for which every ERM hypothesis \(h_S\) satisfies
\[
L_{(\mathcal{D}, f)}(h_S) \leq \varepsilon.
\]

In other words, for a sufficiently large \(m\), the learner
\(\text{ERM}_{\mathcal{H}}\) will be \emph{probably} (with a confidence of at
least \(1 - \delta\)) \emph{approximately} (up to an error of \(\varepsilon\))
\emph{correct} (PAC).
\end{corollary}

\section{PAC Learning Model}

\begin{definition}[PAC learning]
\label{def:pac-learning}
A hypothesis class \(\mathcal{H}\) is said to be \emph{PAC learnable} if there
exists a map
\[
m_{\mathcal{H}}: (0, 1)^2 \longrightarrow \N,
\]
and a learning algorithm \(A\) such that for every tuple
\((\varepsilon, \delta, f, \mathcal{D}, S)\) where
\begin{itemize}\setlength\itemsep{0em}
\item \(\varepsilon, \delta \in (0, 1)\) are the accuracy and confidence
  parameters, respectively.
\item \(f: \mathcal{X} \to \{0, 1\}\) is a binary labelling map on \(\mathcal{X}\).
\item \(\mathcal{D}\) is a probability distribution on \(\mathcal{X}\).
\item The realisability assumption holds with respect to the triple
  \((\mathcal{H}, \mathcal{D}, f)\).
\item The set \(S = (x_j, f(x_j))_{j=1}^m\) with \(m \geq
  m_{\mathcal{H}}(\varepsilon, \delta)\) is composed of i.i.d.~samples of
  \(\mathcal{X}\) generated by \(\mathcal{D}\) and labelled by \(f\).
\end{itemize}
The algorithm \(A\) returns a predictor \(A(S) = h\) with a probability of at
least \(1 - \delta\), over the choice of \(S\), such that
\[
L_{(\mathcal{D}, f)}(h) \leq \varepsilon.
\]

The map \(m_{\mathcal{H}}: (0, 1)^2 \to \N\) determines the \emph{sample
  complexity} necessary to ensure that the learning algorithm will result in a
PAC solution. We shall impose \(m_{\mathcal{H}}\) to return the minimal sample
complexity such that \(\mathcal{H}\) is PAC learnable with accuracy
\(\varepsilon\) and confidence \(\delta\).
\end{definition}

Rephrasing \cref{cor:finite-hypothesis-pac-learnability} with our new jargon:

\begin{corollary}
\label{cor:finite-hypothesis-pac-learnability-succint}
Every finite hypothesis class \(\mathcal{H}\) is PAC learnable with sample complexity
\[
m_{\mathcal{H}}(\varepsilon, \delta) \leq
\left\lceil \frac{\log(|\mathcal{H}| / \delta)}{\varepsilon} \right\rceil.
\]
\end{corollary}

\subsection{Agnostic PAC Learning Model}

Let \(\mathcal{X}\) be our domain space of features and \(\mathcal{Y}\) be the
space of labels. We shall now consider \(\mathcal{D}\) to be the joint
distribution over the product space \(\mathcal{X} \times \mathcal{Y}\). This
allows for two samples corresponding to the same point \(x \in \mathcal{X}\) to
assume different labels \(y_1, y_2 \in \mathcal{Y}\). With this in mind, we need
to revise the true error to be in accordance with this new distribution
\(\mathcal{D}\).

\begin{definition}[Revising the true error]
\label{def:true-error-revised}
Given a probability distribution \(\mathcal{D}\) over \(\mathcal{X} \times
\mathcal{Y}\), and a predictor \(h: \mathcal{X} \to \mathcal{Y}\), the
\emph{true error} of \(h\) is given by
\[
L_{\mathcal{D}}(h) \coloneq \Prob_{(x, y) \sim \mathcal{D}}(h(x) \neq y)
= \mathcal{D}(\{(x, y) \in \mathcal{X} \times \mathcal{Y} \colon h(x) \neq y\}).
\]
\end{definition}

\begin{proposition}[Bayes optimal predictor]
\label{prop:bayes-optimal-predictor}
Let \(\mathcal{D}\) be any distribution on \(\mathcal{X} \times \{0, 1\}\). The
best predictor \(f_{\mathcal{D}}: \mathcal{X} \to \{0, 1\}\), called the
\emph{Bayes optimal predictor}\footnote{In reality, this predictor cannot be
  used since the learner does not have access to the probability distribution
  \(\mathcal{D}\).}, will be given by
\[
f_{\mathcal{D}}(x) \coloneq
\begin{cases}
  1, &\text{if } \Prob_{(x, y) \sim \mathcal{D}}(y = 1 \mid x) \geq 1/2 \\
  0, &\text{otherwise.}
\end{cases}
\]
That is, for any predictor \(h: \mathcal{X} \to \{0, 1\}\) we have
\(L_{\mathcal{D}}(f_{\mathcal{D}}) \leq L_{\mathcal{D}}(h)\).
\end{proposition}

\begin{definition}[Agnostic PAC learning model]
\label{def:agnostic-pac-learning-model}
A hypothesis class \(\mathcal{H}\) is said to be \emph{agnostic PAC learnable}
if there exists a minimal map \(m_{\mathcal{H}}: (0, 1)^2 \to \N\) and leaning
algorithm \(A\) such that for every tuple \((\varepsilon, \delta,
\mathcal{D}, S)\) where
\begin{itemize}\setlength\itemsep{0em}
\item \(\varepsilon, \delta \in (0, 1)\) are the accuracy and confidence
  parameters, respectively.
\item \(\mathcal{D}\) is a probability distribution over
  \(\mathcal{X} \times \mathcal{Y}\).
\item \(S \in (\mathcal{X} \times \mathcal{Y})^m\) is an i.i.d.~sample of size
  \(m \geq m_{\mathcal{H}}(\varepsilon, \delta)\) generated by \(\mathcal{D}\).
\end{itemize}
The algorithm \(A\) returns a predictor \(A(S) \coloneq g \in \mathcal{H}\) with
a probability of at least \(1 - \delta\) over the choice of \(S\), such that
\[
L_{\mathcal{D}}(g) \leq \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h) + \varepsilon.
\]
\end{definition}

\section{Extending The PAC Model}

\begin{definition}[Generalised loss function]
\label{def:generalised-loss-function}
Let \(\mathcal{H}\) be a collection of hypothesis, and \(\mathcal{Z}\) a domain
of interest. Any map of the type
\[
\ell: \mathcal{H} \times \mathcal{Z} \to \R_{\geq 0}
\]
is a \emph{loss function}.
\end{definition}

\begin{definition}[Generalised error functions]
\label{def:generalised-error-functions}
Consider a hypothesis class \(\mathcal{H}\), a domain \(\mathcal{Z}\) with
associated distribution \(\mathcal{D}\), and a loss function \(\ell: \mathcal{H}
\times \mathcal{Z} \to \R_{\geq 0}\). We define the following:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item The \emph{true error} of a predictor \(h \in \mathcal{H}\) is given by
  \[
  L_{\mathcal{D}}(h) \coloneq \Expect_{z \sim \mathcal{D}}[\ell(h, z)].
  \]
\item Given an i.i.d.~sample \(S \in \mathcal{Z}^m\), we define the \emph{empirical
    error} of a predictor \(h \in \mathcal{H}\) to be
  \[
  L_S(h) \coloneq \frac{1}{m} \sum_{z \in S} \ell(h, z)
  \]
\end{enumerate}
\end{definition}

\begin{example}
\label{exp:loss-functions}
The following are extensively used loss functions:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item (\emph{Classifier loss}) Given a domain \(\mathcal{Z} = \mathcal{X} \times
  \mathcal{Y}\) and a predictor \(h\), we define the classifier loss function
  \[
  \ell_{\text{class}}(h, (x, y)) \coloneq
  \begin{cases}
    0, &\text{if } h(x) = y \\
    1, &\text{if } h(x) \neq y.
  \end{cases}
  \]

\item (\emph{Square loss}) Consider a domain \(\mathcal{Z} = \mathcal{X} \times
  \mathcal{Y}\) and a predictor \(h\), we define the square loss function as
  \[
  \ell_{\text{sq}}(h, (x, y)) \coloneq (h(x) - y)^2,
  \]
  which is commonly used in regression learning problems.
\end{enumerate}
\end{example}

\begin{definition}[Agnostic PAC learning model for generalised loss functions]
\label{def:agnostic-pac-learning-model-loss-function}
A hypothesis class \(\mathcal{H}\) is said to be \emph{agnostic PAC learnable}
with respect to a domain \(\mathcal{Z}\) and loss function
\(\ell: \mathcal{H} \times \mathcal{Z} \to \R_{\geq 0}\) if there exists a
minimal map \(m_{\mathcal{H}}: (0, 1)^2 \to \N\) and leaning algorithm \(A\)
such that for every tuple \((\varepsilon, \delta, \mathcal{D}, S)\) where
\begin{itemize}\setlength\itemsep{0em}
\item \(\varepsilon, \delta \in (0, 1)\) are the accuracy and confidence
  parameters, respectively.
\item \(\mathcal{D}\) is a probability distribution over
  \(\mathcal{Z}\).
\item \(S \in \mathcal{Z}^m\) is an i.i.d.~sample of size
  \(m \geq m_{\mathcal{H}}(\varepsilon, \delta)\) generated by \(\mathcal{D}\).
\end{itemize}
The algorithm \(A\) returns a predictor \(A(S) \coloneq g \in \mathcal{H}\) with
a probability of at least \(1 - \delta\) over the choice of \(S\), such that
\[
L_{\mathcal{D}}(g) \leq \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h) + \varepsilon,
\]
where the true error function \(L_{\mathcal{D}}\) is given by
\(L_{\mathcal{D}}(h) = \Expect_{z \sim \mathcal{D}}[\ell(h, z)]\).
\end{definition}

\section{Learning via Uniform Convergence}

\begin{definition}[\(\varepsilon\)-representative sample]
\label{def:epsilon-representative-sample}
Let \(\mathcal{Z}\) be a domain with distribution \(\mathcal{D}\), and
associated hypothesis class \(\mathcal{H}\) and loss function \(\ell\). We say
that a sample \(S\) is \(\varepsilon\)-representative if for every predictor \(h
\in \mathcal{H}\) we have
\[
|L_S(h) - L_{\mathcal{D}}(h)| \leq \varepsilon.
\]
\end{definition}

\begin{lemma}
\label{lem:epsilon-representative-good-erm-output}
Consider a context \((\mathcal{Z}, \mathcal{D}, \mathcal{H}, \ell)\), and an
\((\varepsilon/2)\)-representative sample \(S\). Any output of the ERM learner
\[
\text{ERM}_{\mathcal{H}}(S) = h_S \in \argmin_{h \in \mathcal{H}} L_S(h)
\]
will be such that
\[
L_{\mathcal{D}}(h_S) \leq \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h) + \varepsilon.
\]
\end{lemma}

\begin{corollary}
\label{cor:erm-is-agnostic-pac-learner-sufficient-condition}
The \(\text{ERM}_{\mathcal{H}}\) learner is an agnostic PAC learner if with a
probability of at least \(1- \delta\) over the random choice of a sample \(S\),
the training set \(S\) is \(\varepsilon\)-representative.
\end{corollary}

\begin{definition}[Uniform convergence property]
\label{def:learner-uniform-convergence-property}
A hypothesis class \(\mathcal{H}\) is said to \emph{satisfy the uniform convergence
property}---with respect to a domain \(\mathcal{Z}\) and a loss function
\(\ell\)---if there exists a minimal function
\[
m_{\mathcal{H}}^{\text{UC}}: (0, 1)^2 \longrightarrow \N
\]
such that: for every distribution \(\mathcal{D}\) over \(\mathcal{Z}\), and
parameters \(\varepsilon, \delta \in (0, 1)\), if \(S \in \mathcal{Z}^m\) is an
i.i.d.~sample with size
\(m \geq m_{\mathcal{H}}^{\text{UC}}(\varepsilon, \delta)\), then with a
probability of at least \(1 - \delta\) the sample \(S\) is
\(\varepsilon\)-representative.
\end{definition}

\begin{corollary}
\label{cor:unif-conv-hypothesis-is-agnostically-pac-learnable}
If a hypothesis class \(\mathcal{H}\) has the uniform convergence property---with
respect to \((\mathcal{Z}, \ell)\)---with a function
\(m_{\mathcal{H}}^{\text{UC}}\) then \(\mathcal{H}\) is agnostically PAC
learnable with a sample complexity
\[
m_{\mathcal{H}}(\varepsilon, \delta) \leq
m_{\mathcal{H}}^{\text{UC}}(\varepsilon/2, \delta).
\]
\end{corollary}

\section{Finite Hypothesis Classes are Uniform Convergent}

\begin{proposition}[Strong law of large numbers]
\label{prop:strong-law-of-large-numbers}
Let \((Z_1, \dots, Z_m)\) be a sequence of i.i.d.~random variables with equal
mean \(\mu\). The strong law of large numbers states that when \(m \to \infty\)
the empirical average \(\overline Z \coloneq \frac{1}{m} \sum_{j=1}^m Z_j\)
converges to the expected value \(\mu\) with a probability of \(1\).
\end{proposition}

\begin{lemma}[Markov's inequality]
\label{lem:markov-inequality}
Let \(Z\) be a non-negative random variable. Then for any \(a \geq 0\) we have
\[
\Prob(Z \geq a) \leq \Expect[Z] / a,
\]
known as the Markov's inequality.
\end{lemma}

\begin{lemma}
\label{lem:markov-inequality-lemma}
Given a random variable \(Z\) with values in \([0, 1]\), denote \(\mu \coloneq
\Expect[Z]\). Then for any \(a \in (0, 1)\) we have the following two upper-bounds
\begin{gather}
  \Prob(Z > 1 - a) \geq \frac{\mu - (1 - a)}{a}, \\
  \Prob(Z > a) \geq \frac{\mu - a}{1 - a} \geq \mu - a.
\end{gather}
\end{lemma}

\begin{lemma}[Hoeffding's lemma]
\label{lem:hoeffding-lemma}
Let \(X\) be a random variable taking values in the interval \([a, b]\),
and such that \(\Expect[X] = 0\). Then for every \(\lambda > 0\) we have
\[
\Expect[e^{\lambda X}] \leq \exp\Big( \frac{\lambda^2 (b - a)^2}{8} \Big).
\]
\end{lemma}

\begin{lemma}[Hoeffding's inequality]
\label{lem:hoeffding-inequality}
Let \((Z_1, \dots, Z_m)\) be a collection of i.i.d.~random variables,
and let \(\overline Z \coloneq \frac{1}{m} \sum_{j=1}^m Z_j\).  If for
all \(1 \leq j \leq m\) we have \(\Expect[Z_j] = \mu\) and
\(\Prob[a \leq Z_j \leq b] = 1\), then for any \(\varepsilon > 0\) we have
an upper-bound
\[
\Prob\big( \big| \overline{Z} - \mu \big| > \varepsilon \big)
\leq 2 \exp\Big( -\frac{2 m \varepsilon^{2}}{(b - a)^2} \Big).
\]
\end{lemma}

\begin{proposition}
\label{prop:finite-hypothesis-agnostically-pac-learnable}
Let \(\mathcal{H}\) be a \emph{finite} hypothesis class with respect to a domain
\(\mathcal{Z}\), and let \(\ell: \mathcal{H} \times \mathcal{Z} \to [0, 1]\) be
a loss function. Then \(\mathcal{H}\) has the uniform convergence property with
a sample complexity of
\[
m_{\mathcal{H}}^{\text{UC}}(\varepsilon, \delta) \leq
\left\lceil \frac{\log(2 |\mathcal{H}| / \delta)}{2 \varepsilon^2} \right\rceil.
\]
Moreover, \(\mathcal{H}\) is agnostically PAC learnable with respect to the ERM
algorithm with a sample complexity
\[
m_{\mathcal{H}}(\varepsilon) \leq m_{\mathcal{H}}^{\text{UC}}(\varepsilon/2, \delta)
\leq
\left\lceil \frac{2 \log(2 |\mathcal{H}| / \delta)}{\varepsilon^2} \right\rceil
\]
\end{proposition}

\section{Universal Learners are Impossible}

\begin{theorem}[No-free-lunch]
\label{thm:no-free-lunch}
Let \(A\) be a learning algorithm assigned for the binary classification of a
domain \(\mathcal{X}\) with respect to the \(\{0, 1\}\)-loss function
\(\ell_{\text{class}}\). Let \(m < |\mathcal{X}|/2\) be the size of a training
set. There exists a distribution \(\mathcal{D}\) over the domain
\(\mathcal{X} \times \{0, 1\}\) for which
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item There exists a labelling map \(f: \mathcal{X} \to \{0, 1\}\) for which
  \(L_{\mathcal{D}}(f) = 0\).
\item With a probability of at least \(1/7\) over the choice of the training set
  \(S \sim \mathcal{D}^m\) we have
  \[
  L_{\mathcal{D}}(A(S)) \geq \frac{1}{8}.
  \]
\end{enumerate}
In other words, there exists a true labelling map \(f\), with respect to the
distribution \(\mathcal{D}\), underlying the learning problem, and the algorithm
\emph{fails} to output a predictor with a good approximation of \(f\).
\end{theorem}

% \begin{proof}
% Let \(C \subseteq \mathcal{X}\) be a subset of size \(2 m\). We'll prove that if
% a learner has access to only half of \(C\), then it has no information on how to
% correctly label the remaining half of \(C\). The number of maps
% \(C \to \{0, 1\}\) is given by \(T \coloneq |\{0, 1\}|^{|C|} = 2^{2m}\): let
% \((f_1, \dots, f_T)\) denote all such maps. For each \(1 \leq j \leq T\) define
% a distribution \(\mathcal{D}_j\) over \(C \times \{0, 1\}\) where
% \[
% \mathcal{D}_j(\{(x, y)\}) =
% \begin{cases}
%   \frac{1}{|C|}, &\text{if } y = f_i(x) \\
%   0, &\text{otherwise.}
% \end{cases}
% \]
% That is, \(\mathcal{D}_j\) makes \(f_j\) the true labelling function on
% \(C\)---so that \(L_{\mathcal{D}_j}(f_j) = 0\).

% Let \(K = (2 m)^m\) denote the number of possible sequences consisting of \(m\)
% i.i.d.~sampled instances from \(C\), and let \((S_1, \dots, S_K)\) denote all
% such sequences. Given a sequence \(S_j = (x_1, \dots, x_m)\), define
% \(S_j^i \coloneq (x_j, f_j)_{j=1}^m\) to be the labelling of \(S_j\) by the map
% \(f_i\). Given a distribution \(D_i\), the available training sets for the
% algorithm \(A\) are \((S_1^i, \dots, S_K^i)\), which are i.i.d.~and labelled by
% the same map \(f_i\). Therefore one has
% \begin{equation}\label{eq:nfl-expect-AS}
% \Expect_{S \sim \mathcal{D}_i^m}[L_{\mathcal{D}_i}(A(S))]
% = \frac{1}{K} \sum_{j=1}^K L_{\mathcal{D}_i}(A(S_{j}^i)).
% \end{equation}

% Considering all \(T\) pairs \((f_j, \mathcal{D}_j)\) we have the following
% relations:
% \begin{align}\label{eq:nfl-max-i-in-T}
%   \nonumber
%   \max_{1 \leq i \leq T} \Expect_{S \sim \mathcal{D}_i^m}[L_{\mathcal{D}_i}(A(S))]
%   &\geq \frac{1}{T} \sum_{i=i}^{T} \Expect_{S \sim \mathcal{D}_i^m}[L_{\mathcal{D}_i}(A(S))]
%   \\
%   \nonumber
%   &= \frac{1}{T} \sum_{i=i}^{T} \Big(
%     \frac{1}{K} \sum_{j=1}^K L_{\mathcal{D}_i}(A(S_j^i))
%     \Big) \\
%   \nonumber
%   &= \frac{1}{K} \sum_{j=1}^{K} \Big(
%     \frac{1}{T} \sum_{i=1}^T L_{\mathcal{D}_i}(A(S_j^i))
%     \Big) \\
%   &\geq \min_{1 \leq j \leq K} \frac{1}{T} \sum_{i=1}^T L_{\mathcal{D}_i}(A(S_j^i)).
% \end{align}
% Fix any \(1 \leq j \leq K\) and let \(S_j \coloneq (x_1, \dots, x_m)\) and let
% \((v_1, \dots, v_p)\) be a sequence containing all instances of \(C\) not
% appearing in \(S_j\)---which certainly has \(p \geq m\). Hence, for any
% labelling \(h: C \to \{0, 1\}\) and any \(1 \leq i \leq T\) we have a true error
% \begin{align*}
% L_{\mathcal{D}_i}(h)
% &= \frac{1}{2m} \sum_{x \in C} \ell_{\text{class}}(h, (x, f_i(x))) \\
% &\geq \frac{1}{2 m} \sum_{k=1}^p \ell_{\text{class}}(h, (v_k, f_i(v_k))) \\
% &\geq \frac{1}{2 p} \sum_{k=1}^p \ell_{\text{class}}(h, (v_k, f_i(v_k))).
% \end{align*}
% From this we obtain that
% \begin{align}\label{eq:nfl-avg-true-loss-T}
%   \nonumber
%   \frac{1}{T} \sum_{i=1}^T L_{\mathcal{D}_i}(A(S_j^i))
%   &\geq \frac{1}{T} \sum_{i=1}^T \Big(
%     \frac{1}{2 p} \sum_{k=1}^p \ell_{\text{class}}(A(S_j^i), (v_k, f_i(v_k)))
%     \Big) \\
%   \nonumber
%   &= \frac{1}{2 p} \sum_{k=1}^p \Big(
%     \frac{1}{T} \sum_{i=1}^T \ell_{\text{class}}(A(S_j^i), (v_k, f_i(v_k)))
%     \Big) \\
%   &\geq \frac{1}{2} \min_{1 \leq k \leq p} \frac{1}{T}
%     \sum_{i=1}^T \ell_{\text{class}}(A(S_j^i), (v_k, f_i(v_k))).
% \end{align}
% For the last time, fix an index \(1 \leq k \leq p\). Partition \((f_1, \dots,
% f_T)\) into a collection of \(T/2\) disjoint pairs of the form \((f_i, f_{i'})\)
% for which \(f_i(c) \neq f_{i'}(c)\) if and only if \(c = v_k\). From
% construction, such pairs satisfy \(S_j^i = S_j^{i'}\) for any \(1 \leq j \leq
% K\), therefore
% \[
% \ell_{\text{class}}(A(S_j^i), (v_k, f_i(v_k)))
% + \ell_{\text{class}}(A(S_j^{i'}), (v_k, f_{i'}(v_k)))
% = 1,
% \]
% which in turn implies in
% \begin{equation}\label{eq:nfl-avg-T-is-half}
% \frac{1}{T} \sum_{i=1}^T \ell_{\text{class}}(A(S_j^i), (v_k, f_i(v_k)) = \frac{1}{2}.
% \end{equation}

% Now substituting \cref{eq:nfl-avg-T-is-half} in \cref{eq:nfl-avg-true-loss-T}
% results in
% \[
% \frac{1}{T} \sum_{i=1}^{T} L_{\mathcal{D}_i}(A(S_j^i)) \geq \frac{1}{4}.
% \]
% If we now substitute this into \cref{eq:nfl-max-i-in-T} we get
% \[
% \max_{1 \leq i \leq T} \Expect_{S \sim \mathcal{D}_i^m}[L_{\mathcal{D}_i}(A(S))]
% \geq \frac{1}{4},
% \]
% and say this maximum is attained at an index \(1 \leq i_0 \leq T\). Denote
% \((f_{i_0}, \mathcal{D}_{i_0}) \coloneq (f, \mathcal{D})\) for short. Using the
% \cref{lem:markov-inequality-lemma} we have
% \[
% \Prob(L_{\mathcal{D}}(A(S)) \geq 1/8)
% \geq \frac{\Expect_{S \sim \mathcal{D}^m}[L_{\mathcal{D}}(A(S))] - 1/8}{1 - 1/8}
% = \frac{1}{7}
% \]
% \end{proof}

\begin{corollary}
\label{cor:full-hypothesis-class-not-pac-learnable}
Let \(\mathcal{X}\) be an infinite domain and define
\(\mathcal{H} \coloneq \{0, 1\}^{\mathcal{X}}\) to be our hypothesis class---that is, the collection of all
possible binary classifiers of \(\mathcal{X}\). Then
\(\mathcal{H}\) is \emph{not} PAC learnable.
\end{corollary}

\begin{definition}[Restricting the hypothesis class]
\label{def:restrict-hypothesis-class}
Let \(\mathcal{H}\) be a hypothesis class of maps \(\mathcal{X} \to \mathcal{Y}\) and take a finite set
\(C \subseteq \mathcal{X}\). We define \(\mathcal{H}|_C \coloneq (h|_C)_{h \in \mathcal{H}}\) to be the restriction of the
hypothesis class to \(C\). We say that \(\mathcal{H}\) \emph{shatters} \(C\) if it is the
case that \(\mathcal{H}|_C = \mathcal{Y}^{C}\).
\end{definition}

\begin{corollary}
\label{cor:shatters-2m-cannot-learn-with-train-of-size-m}
Let \(\mathcal{H}\) be a hypothesis class of maps \(\mathcal{X} \to \{0, 1\}\), and \(m\) be the
training set size. Assume the existence of a subset \(C \subseteq \mathcal{X}\) with cardinality
\(2 m\) such that \(\mathcal{H}|_C\) shatters \(C\). It follows that for any learning
algorithm \(A\) there exists a distribution \(\mathcal{D}\) over \(\mathcal{X} \times \{0, 1\}\) and a
predictor \(h \in \mathcal{H}\) for which \(L_{\mathcal{D}}(h) = 0\) and having a probability of at
least \(1/7\) over the choice of samples \(S \sim \mathcal{D}^m\) that \(L_{\mathcal{D}}A(S) \geq 1/8\).
\end{corollary}

\begin{definition}[VC-dimension]
\label{def:vc-dimension}
Let \(\mathcal{H}\) be a hypothesis class. We define the \emph{VC-dimension} of
\(\mathcal{H}\) to be:
\[
\vcdim \mathcal{H} \coloneq \max \{|C| \colon C \subseteq \mathcal{X} \text{ is shattered by } \mathcal{H}|_C\}.
\]
\end{definition}

\begin{theorem}
\label{thm:infinite-VC-dim-not-learnable}
If \(\mathcal{H}\) is a class with \emph{infinite} VC-dimension, then
\(\mathcal{H}\) is \emph{not PAC learnable}.
\end{theorem}

\begin{proof}
Let \(S\) be a training set with size \(m\). Since
\(\vcdim \mathcal{H} = \infty\) then there exists a set with size \(2 m\) that is shattered by
\(\mathcal{H}\)---thus by \cref{cor:shatters-2m-cannot-learn-with-train-of-size-m}
\(\mathcal{H}\) isn't PAC learnable.
\end{proof}

\section{Fundamental Theorem of PAC Learning}

\begin{definition}[Growth function]
\label{def:growth-function-learning}
Let \(\mathcal{H}\) be a hypothesis class. We define the growth function of \(\mathcal{H}\) to be
the map \(\Growth_{\mathcal{H}}: \N \to \N\) given by
\[
\Growth_{\mathcal{H}}(m) \coloneq \max \{|\mathcal{H}_C| \colon C \subseteq \mathcal{X} \text{ with } |C| = m\}.
\]
\end{definition}

\begin{corollary}
\label{cor:growth-function-lower-vc-dim}
If \(\vcdim \mathcal{H} = d\) then for any \(m \leq d\) we have \(\Growth_{\mathcal{H}}(m) = 2^m\).
\end{corollary}

\begin{lemma}[Sauer-Shelah-Perles]
\label{lem:sauer-shelah-perles}
Let \(\mathcal{H}\) be a hypothesis class with \(\vcdim \mathcal{H} \leq d\) finite. Then for any
\(m \in \N\) one has
\[
\Growth_{\mathcal{H}}(m) \leq \sum_{j=0}^d \binom m j.
\]
In particular, if \(m > d + 1\) then our bound becomes
\[
\Growth_{\mathcal{H}}(m) \leq \Big(\frac{e m}{d}\Big)^d
\]
\end{lemma}

\begin{theorem}[Fundamental theorem of statistical learning]
\label{thm:fundamental-thm-stat-learning}
Let \(\mathcal{H}\) be a hypothesis class of maps \(\mathcal{X} \to \{0, 1\}\) together with the
binary loss function. The following conditions are equivalent:
\begin{enumerate}[(a)]\setlength\itemsep{0em}
\item The collection \(\mathcal{H}\) has the uniform convergence property.

\item Any ERM rule succeeds at PAC learning \(\mathcal{H}\) agnostically.

\item The collection \(\mathcal{H}\) is agnostic PAC learnable.

\item The collection \(\mathcal{H}\) is PAC learnable.

\item Any ERM rule succeeds at PAC learning \(\mathcal{H}\).

\item The collection \(\mathcal{H}\) has a finite VC-dimension.
\end{enumerate}
\end{theorem}

% \section{Non-Uniform Learnability}

% \begin{definition}[Competitive hypothesis]
% \label{def:competitive-hypothesis}
% Let \(\mathcal{H}\) be a hypothesis class, and let \(h, h' \in \mathcal{H}\) be a pair of predictors,
% and consider parameters \(\varepsilon, \delta \in (0, 1)\). We say that \(h\) is
% \((\varepsilon, \delta)\)-competitive with respect to \(h'\) if with a probability of at least
% \(1 - \delta\) we have
% \[
% L_{\mathcal{D}}(h) \leq L_{\mathcal{D}}(h') + \varepsilon.
% \]
% \end{definition}

% \begin{definition}
% \label{def:non-uniform-learnable-hypothesis-class}
% We say that a hypothesis class \(\mathcal{H}\) is \emph{non-uniformly learnable} if there
% exists an algorithm \(A\) and a map
% \[
% m_{\mathcal{H}}^{\text{NUL}}: (0, 1)^2 \times \mathcal{H} \longrightarrow \N
% \]
% such that, for every triple \((\varepsilon, \delta, h) \in (0, 1)^2 \times \mathcal{H}\) the resulting predictor
% \(A(S)\) is \((\varepsilon, \delta)\)-competitive with respect to \(h\). That is, with a
% probability of at least \(1 - \delta\) over the choice of sample set \(S \sim \mathcal{D}^m\) we have
% \[
% L_{\mathcal{D}} A(S) \leq L_{\mathcal{D}}(h) + \varepsilon.
% \]
% \end{definition}

% \begin{lemma}
% \label{lem:countable-unif-conv-prop--then-non-unif-learnable}
% Let \(\mathcal{H} = \bigcup_{n \in \N} \mathcal{H}_n\) be a hypothesis class. If each \(\mathcal{H}_n\) has the
% uniform convergence property, then \(\mathcal{H}\) is non-uniformly learnable.
% \end{lemma}

% \begin{theorem}
% \label{thm:non-uniformly-learnable-iff-countable-union-pac}
% Let \(\mathcal{H}\) be a hypothesis class of maps of the form
% \(\mathcal{X} \to \{0, 1\}\). The class \(\mathcal{H}\) is non-uniformly learnable if and only if it
% consists of a countable union of agnostic PAC learnable hypothesis classes.
% \end{theorem}

\section{Next Semester}

The following table displays the planned courses that the student will
participate in the following semester.

\begin{table}[h!]\label{tab:disc}
    \centering
    \caption{Planned list of courses that will be attended in the next semester (second semester of 2023).}
    \begin{tabular}{ |c|c|c|c|c| }
        \hline
        Year and Semester & Code & Discipline & Type & Credits \\
        \hline
        \multirow{4}{*}{2023 / 2}
              & CCM0418 & Iniciação à Pesquisa III     & G  & 12      \\
              & MAT5008 & Teoria de Homotopia Abstrata & PG & 8 \\
              & MAC5922 & Combinatória I               & PG & 8 \\
              & MAC5921 & Deep Learning                & PG & 8 \\
        \hline
    \end{tabular}
\end{table}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
